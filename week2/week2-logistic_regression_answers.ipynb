{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfee4b65",
   "metadata": {},
   "source": [
    "\n",
    "# MLF Week 2: Logistic Regression, Model Evaluation, and Autograd\n",
    "\n",
    "This notebook accompanies the **MLF Week 2 Slides**, with a content focuses on **binary classification with logistic regression**, **model evaluation**, and **using PyTorch `nn` and `optim`**.\n",
    "\n",
    "**You will:**\n",
    "- Understand sigmoid and decision boundaries\n",
    "- Implement Binary Cross-Entropy (BCE) and compare with PyTorch\n",
    "- Split data into **train**, **validation**, and **test** sets\n",
    "- Build a logistic regression classifier using `torch.nn` and `torch.optim`\n",
    "- Plot a decision boundary for a 2D dataset\n",
    "- Diagnose **overfitting vs underfitting** and try **L2 regularization**\n",
    "- Complete graded mini-exercises with tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12031136",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.8.0\n",
    "%pip install matplotlib==3.10.6\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338aa30",
   "metadata": {},
   "source": [
    "\n",
    "# 1. From Linear to Logistic\n",
    "\n",
    "**Classification** predicts a **class label**. We start with a linear score then squash it to a probability using the **sigmoid** function:\n",
    "\n",
    "\\begin{equation}\n",
    "z = w^\\top x + b,\\qquad \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
    "\\end{equation}\n",
    "\n",
    "The model predicts class 1 when $\\sigma(z) \\ge 0.5$ and class 0 otherwise. The line where $\\sigma(z)=0.5$ defines the **decision boundary**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b5cf4",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Exercise: Implement `sigmoid`\n",
    "\n",
    "Fill in the TODO to implement a **numerically stable** sigmoid. Avoid overflow for large negative inputs.\n",
    "\n",
    "**Hints**\n",
    "- For $z \\ge 0$: $\\sigma(z) = 1 / (1 + e^{-z})$\n",
    "- For $z < 0$: $\\sigma(z) = e^{z} / (1 + e^{z})$\n",
    "\n",
    "We will test your function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_sigmoid(z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically stable sigmoid.\n",
    "    Args:\n",
    "        z: tensor of logits\n",
    "    Returns:\n",
    "        probabilities in (0, 1)\n",
    "    \"\"\"\n",
    "    # TODO: implement a numerically stable sigmoid (no torch.sigmoid call)\n",
    "    # Replace the next line with your solution.\n",
    "    pos_mask = (z >= 0)\n",
    "    neg_mask = ~pos_mask\n",
    "    out = torch.empty_like(z)\n",
    "\n",
    "    # Positive branch: 1 / (1 + exp(-z))\n",
    "    out[pos_mask] = 1.0 / (1.0 + torch.exp(-z[pos_mask]))\n",
    "    # Negative branch: exp(z) / (1 + exp(z))\n",
    "    ez = torch.exp(z[neg_mask])\n",
    "    out[neg_mask] = ez / (1.0 + ez)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78659eda",
   "metadata": {},
   "source": [
    "\n",
    "### Tests: `student_sigmoid`\n",
    "These should pass if your implementation is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f85e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import grade_sigmoid, show_result\n",
    "\n",
    "res = grade_sigmoid(student_sigmoid)\n",
    "show_result(\"student_sigmoid\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd2ba5",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Binary Cross-Entropy (BCE) Loss\n",
    "\n",
    "For binary labels $y \\in \\{0,1\\}$ and predicted probability $ \\hat{p} = \\sigma(z) $, the **BCE** loss is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\text{BCE}}(y,\\hat{p}) = -\\Big[y\\log(\\hat{p}) + (1-y)\\log(1-\\hat{p})\\Big].\n",
    "\\end{equation}\n",
    "\n",
    "In practice we often pass logits $z$ directly to a numerically stable function like `nn.BCEWithLogitsLoss`, which internally applies sigmoid and BCE in one go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765b866",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 Exercise: Implement `binary_cross_entropy`\n",
    "\n",
    "Implement BCE given **probabilities** $\\hat{p}$ and labels $y$. Use a small `eps` to avoid $\\log(0)$. We will compare against PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bde85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(probs: torch.Tensor, targets: torch.Tensor, eps: float = 1e-7) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    BCE computed from probabilities.\n",
    "    Args:\n",
    "        probs: predicted probabilities in (0, 1), shape [N] or [N,1]\n",
    "        targets: binary labels 0/1, same shape as probs\n",
    "    Returns:\n",
    "        mean BCE loss (scalar tensor)\n",
    "    \"\"\"\n",
    "    # TODO: implement BCE safely using eps\n",
    "    probs = probs.clamp(eps, 1.0 - eps)\n",
    "    loss = -(targets * torch.log(probs) + (1.0 - targets) * torch.log(1.0 - probs))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0f66e",
   "metadata": {},
   "source": [
    "\n",
    "### Tests: `binary_cross_entropy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ad252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import grade_bce, show_result\n",
    "\n",
    "res = grade_bce(binary_cross_entropy)\n",
    "show_result(\"binary_cross_entropy\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c090e",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Train, Validation, Test Split\n",
    "\n",
    "We split the dataset to estimate generalization and to tune hyperparameters without contaminating the test set.\n",
    "\n",
    "- **Train**: optimize parameters\n",
    "- **Validation**: choose hyperparameters and detect overfitting\n",
    "- **Test**: final unbiased estimate\n",
    "\n",
    "Train optimizes parameters. Validation selects hyperparameters and detects overfitting. The test set is never touched until the very end, otherwise you leak information and inflate the final estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b77b3",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 Generate a 2D synthetic dataset\n",
    "\n",
    "We create two Gaussian blobs that are (roughly) linearly separable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a50c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_blobs(n_per_class=200, centers=((0.0, 0.0), (2.5, 2.5)), std=0.8, seed=42):\n",
    "    gen = torch.Generator().manual_seed(seed)\n",
    "    c0 = torch.randn(n_per_class, 2, generator=gen) * std + torch.tensor(centers[0])\n",
    "    c1 = torch.randn(n_per_class, 2, generator=gen) * std + torch.tensor(centers[1])\n",
    "    X = torch.cat([c0, c1], dim=0)\n",
    "    y = torch.cat([torch.zeros(n_per_class, 1), torch.ones(n_per_class, 1)], dim=0)\n",
    "    idx = torch.randperm(len(X), generator=gen)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "X, y = make_blobs()\n",
    "print(\"X shape:\", X.shape, \"y mean:\", y.float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70c07d",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 Exercise: Implement `train_val_test_split`\n",
    "\n",
    "Write a function that splits tensors `X, y` into `(Xtr, ytr), (Xva, yva), (Xte, yte)` given ratios.\n",
    "\n",
    "**Requirements**\n",
    "- Use a fixed seed for reproducibility\n",
    "- No overlap between splits\n",
    "- Return contiguous tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X: torch.Tensor, y: torch.Tensor, ratios=(0.7, 0.15, 0.15), seed=123):\n",
    "    \"\"\"\n",
    "    Split X, y into train/val/test.\n",
    "    Returns: (Xtr, ytr), (Xva, yva), (Xte, yte)\n",
    "    \"\"\"\n",
    "    # TODO: implement a reproducible random split\n",
    "    n = len(X)\n",
    "    r_tr, r_va, r_te = ratios\n",
    "    assert abs(r_tr + r_va + r_te - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    idx = torch.randperm(n, generator=g)\n",
    "\n",
    "    n_tr = int(n * r_tr)\n",
    "    n_va = int(n * r_va)\n",
    "    n_te = n - n_tr - n_va\n",
    "\n",
    "    idx_tr = idx[:n_tr]\n",
    "    idx_va = idx[n_tr:n_tr + n_va]\n",
    "    idx_te = idx[n_tr + n_va:]\n",
    "\n",
    "    Xtr, ytr = X[idx_tr].contiguous(), y[idx_tr].contiguous()\n",
    "    Xva, yva = X[idx_va].contiguous(), y[idx_va].contiguous()\n",
    "    Xte, yte = X[idx_te].contiguous(), y[idx_te].contiguous()\n",
    "    return (Xtr, ytr), (Xva, yva), (Xte, yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699cb1fc",
   "metadata": {},
   "source": [
    "\n",
    "### Tests: `train_val_test_split`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c60b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import grade_split, show_result\n",
    "\n",
    "res = grade_split(train_val_test_split, X, y, ratios=(0.6, 0.2, 0.2), seed=7)\n",
    "show_result(\"train_val_test_split\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2e2b0",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Logistic Regression in PyTorch (`nn` + `optim`)\n",
    "\n",
    "We use a single linear layer to produce logits $ z = w^\\top x + b $ and the stable `BCEWithLogitsLoss`. Accuracy uses a threshold at 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adacc9",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1 Exercise: Complete the training loop\n",
    "\n",
    "Fill the TODOs in `train_logreg` for a working training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_dim=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # logits\n",
    "\n",
    "\n",
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= 0.5).float()\n",
    "    return (preds.eq(targets).float().mean().item())\n",
    "\n",
    "\n",
    "def train_logreg(Xtr, ytr, Xva, yva, lr=0.1, weight_decay=0.0, epochs=300):\n",
    "    model = LogisticRegression(in_dim=Xtr.shape[1])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    tr_losses, va_losses, va_accs = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        # TODO: forward on train\n",
    "        logits = model(Xtr)\n",
    "        loss = criterion(logits, ytr)\n",
    "\n",
    "        # TODO: backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            va_logits = model(Xva)\n",
    "            va_loss = criterion(va_logits, yva).item()\n",
    "            va_acc = accuracy_from_logits(va_logits, yva)\n",
    "            va_losses.append(va_loss)\n",
    "            va_accs.append(va_acc)\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"epoch {epoch:03d}  train_loss={loss.item():.4f}  val_loss={va_loss:.4f}  val_acc={va_acc:.3f}\")\n",
    "\n",
    "    return model, tr_losses, va_losses, va_accs\n",
    "\n",
    "\n",
    "# Prepare splits\n",
    "(Xtr, ytr), (Xva, yva), (Xte, yte) = train_val_test_split(X, y, ratios=(0.7, 0.15, 0.15), seed=42)\n",
    "\n",
    "model, tr_losses, va_losses, va_accs = train_logreg(Xtr, ytr, Xva, yva, lr=0.2, weight_decay=0.0, epochs=300)\n",
    "\n",
    "print(f\"Validation accuracy (last): {va_accs[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3dd79d",
   "metadata": {},
   "source": [
    "\n",
    "### Test: Training loop reduces loss\n",
    "\n",
    "Checks that training loss decreases overall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0710c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import grade_training_progress, grade_validation_accuracy, show_result\n",
    "\n",
    "show_result(\"training_progress\", grade_training_progress(tr_losses))\n",
    "show_result(\"validation_accuracy\", grade_validation_accuracy(va_accs, threshold=0.85))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fe74c",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Visualizing the Decision Boundary\n",
    "\n",
    "We visualize a contour where the model predicts 0.5 probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, padding=1.0, steps=200):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_min, x_max = X[:,0].min().item() - padding, X[:,0].max().item() + padding\n",
    "        y_min, y_max = X[:,1].min().item() - padding, X[:,1].max().item() + padding\n",
    "        xs = torch.linspace(x_min, x_max, steps=steps)\n",
    "        ys = torch.linspace(y_min, y_max, steps=steps)\n",
    "        grid_x, grid_y = torch.meshgrid(xs, ys, indexing=\"xy\")\n",
    "        grid = torch.stack([grid_x.reshape(-1), grid_y.reshape(-1)], dim=1)\n",
    "        logits = model(grid)\n",
    "        probs = torch.sigmoid(logits).reshape(steps, steps)\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.contourf(grid_x.numpy(), grid_y.numpy(), probs.numpy(), levels=20, alpha=0.6)\n",
    "        plt.contour(grid_x.numpy(), grid_y.numpy(), (probs.numpy() >= 0.5).astype(float), levels=[0.5])\n",
    "        plt.scatter(X[y.squeeze()==0,0].numpy(), X[y.squeeze()==0,1].numpy(), s=12, label=\"class 0\")\n",
    "        plt.scatter(X[y.squeeze()==1,0].numpy(), X[y.squeeze()==1,1].numpy(), s=12, label=\"class 1\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Decision boundary and data\")\n",
    "        plt.xlabel(\"x1\")\n",
    "        plt.ylabel(\"x2\")\n",
    "        plt.show()\n",
    "\n",
    "plot_decision_boundary(model, Xtr, ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8fa86",
   "metadata": {},
   "source": [
    "\n",
    "# 6. Overfitting vs Underfitting, and L2 Regularization\n",
    "\n",
    "- **Underfitting**: the model is too simple and fails to capture structure.\n",
    "- **Overfitting**: the model memorizes noise and performs poorly on new data.\n",
    "- **Regularization (L2/weight decay)** shrinks parameters and can reduce overfitting.\n",
    "\n",
    "We compare training with and without L2 on a noisy dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea942a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a noisier dataset\n",
    "X2, y2 = make_blobs(n_per_class=150, centers=((0.0, 0.0), (2.2, 2.2)), std=1.2, seed=7)\n",
    "(Xtr2, ytr2), (Xva2, yva2), (Xte2, yte2) = train_val_test_split(X2, y2, seed=3)\n",
    "\n",
    "# No regularization\n",
    "m0, tr0, va0, acc0 = train_logreg(Xtr2, ytr2, Xva2, yva2, lr=0.2, weight_decay=0.0, epochs=250)\n",
    "print(f\"No-reg val acc: {acc0[-1]:.3f}\")\n",
    "plot_decision_boundary(m0, Xtr2, ytr2)\n",
    "\n",
    "# L2 regularization\n",
    "m1, tr1, va1, acc1 = train_logreg(Xtr2, ytr2, Xva2, yva2, lr=0.2, weight_decay=1e-2, epochs=250)\n",
    "print(f\"L2 val acc: {acc1[-1]:.3f}\")\n",
    "plot_decision_boundary(m1, Xtr2, ytr2)\n",
    "\n",
    "# Compare weight norms\n",
    "with torch.no_grad():\n",
    "    w0 = m0.linear.weight.norm().item()\n",
    "    w1 = m1.linear.weight.norm().item()\n",
    "print(f\"Weight norm no-reg: {w0:.3f}  vs  with L2: {w1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512a14f",
   "metadata": {},
   "source": [
    "\n",
    "# 7. Final Evaluation on the Test Set\n",
    "\n",
    "Evaluate your final model on the held-out test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test(model, Xte, yte):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(Xte)\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, yte).item()\n",
    "        acc = accuracy_from_logits(logits, yte)\n",
    "    print(f\"Test loss: {loss:.4f}, Test accuracy: {acc:.3f}\")\n",
    "    return loss, acc\n",
    "\n",
    "_ = evaluate_on_test(model, Xte, yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c8ad9",
   "metadata": {},
   "source": [
    "\n",
    "# 8. Conclusion\n",
    "You now have:\n",
    "- A numerically stable sigmoid and BCE implementation\n",
    "- A clean train / val / test workflow\n",
    "- A PyTorch logistic regression model + monitoring\n",
    "- Experience diagnosing underfitting vs overfitting\n",
    "- Hands‑on exposure to L2 regularization effects\n",
    "\n",
    "**Great job on completing Week 2!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
