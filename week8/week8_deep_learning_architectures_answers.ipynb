{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 8 ‚Äî Deep Learning Architectures (ANSWERS)\n",
        "\n",
        "## Objectives\n",
        "- Understand the evolution of deep learning architectures\n",
        "- Implement AlexNet for image classification\n",
        "- Learn about skip connections and build ResNet blocks\n",
        "- Implement LSTM for time series prediction\n",
        "- Understand the Transformer architecture and attention mechanism\n",
        "- Explore Vision Transformers (ViT)\n",
        "\n",
        "This notebook provides a comprehensive tour through the major deep learning architectures that have shaped modern AI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from utils import (\n",
        "    show_result, generate_image_data, generate_time_series_data,\n",
        "    generate_sequence_classification_data, train_test_split,\n",
        "    Conv2d, MaxPool2d, ReLU, Dropout, Linear, BatchNorm2d,\n",
        "    LSTM, LSTMCell, MultiHeadAttention, PositionalEncoding,\n",
        "    scaled_dot_product_attention, softmax, accuracy, mse,\n",
        "    test_alexnet_architecture, test_resnet_skip_connection,\n",
        "    test_lstm_forward, test_transformer_attention, test_vit_patch_embedding\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Deep Learning Architectures\n",
        "\n",
        "### Brief History\n",
        "- **2012**: AlexNet wins ImageNet, sparking the deep learning revolution\n",
        "- **2015**: ResNet introduces skip connections, enabling very deep networks (152+ layers)\n",
        "- **2017**: Transformers revolutionize NLP with attention mechanisms\n",
        "- **2020**: Vision Transformers (ViT) show transformers can excel at computer vision\n",
        "\n",
        "### Key Architecture Families\n",
        "\n",
        "1. **Convolutional Neural Networks (CNNs)**\n",
        "   - Designed for spatial data (images)\n",
        "   - Use local connectivity and weight sharing\n",
        "   - Examples: LeNet, AlexNet, VGG, ResNet, Inception\n",
        "\n",
        "2. **Recurrent Neural Networks (RNNs)**\n",
        "   - Designed for sequential data (text, time series)\n",
        "   - Maintain hidden state across time steps\n",
        "   - Examples: Vanilla RNN, LSTM, GRU\n",
        "\n",
        "3. **Transformers**\n",
        "   - Use attention mechanisms to process sequences\n",
        "   - Can be parallelized (unlike RNNs)\n",
        "   - Examples: BERT, GPT, T5, ViT\n",
        "\n",
        "### Why Different Architectures?\n",
        "- **Inductive biases**: Built-in assumptions about the data structure\n",
        "- **CNNs** assume spatial locality and translation invariance\n",
        "- **RNNs** assume sequential dependencies\n",
        "- **Transformers** make fewer assumptions, learn patterns from data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. AlexNet: The CNN Revolution\n",
        "\n",
        "AlexNet (2012) was the breakthrough that brought deep learning to mainstream computer vision.\n",
        "\n",
        "### Architecture Overview\n",
        "```\n",
        "Input (224√ó224√ó3)\n",
        "    ‚Üì\n",
        "Conv1 (11√ó11, stride 4) ‚Üí 96 filters ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Conv2 (5√ó5) ‚Üí 256 filters ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Conv3 (3√ó3) ‚Üí 384 filters ‚Üí ReLU\n",
        "    ‚Üì\n",
        "Conv4 (3√ó3) ‚Üí 384 filters ‚Üí ReLU\n",
        "    ‚Üì\n",
        "Conv5 (3√ó3) ‚Üí 256 filters ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Flatten\n",
        "    ‚Üì\n",
        "FC1 (4096) ‚Üí ReLU ‚Üí Dropout\n",
        "    ‚Üì\n",
        "FC2 (4096) ‚Üí ReLU ‚Üí Dropout\n",
        "    ‚Üì\n",
        "FC3 (num_classes)\n",
        "```\n",
        "\n",
        "### Key Innovations\n",
        "1. **ReLU activation**: Faster training than sigmoid/tanh\n",
        "2. **Dropout**: Regularization to prevent overfitting\n",
        "3. **Data augmentation**: Random crops, flips\n",
        "4. **GPU training**: Made deep networks practical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Implement AlexNet (SOLUTION)\n",
        "\n",
        "class AlexNet:\n",
        "    def __init__(self, num_classes=10):\n",
        "        \"\"\"\n",
        "        Initialize AlexNet architecture.\n",
        "        For simplicity, we'll use a slightly smaller version.\n",
        "        \n",
        "        Args:\n",
        "            num_classes: Number of output classes\n",
        "        \"\"\"\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2d(3, 96, kernel_size=11, stride=4, padding=2)\n",
        "        self.conv2 = Conv2d(96, 256, kernel_size=5, padding=2)\n",
        "        self.conv3 = Conv2d(256, 384, kernel_size=3, padding=1)\n",
        "        self.conv4 = Conv2d(384, 384, kernel_size=3, padding=1)\n",
        "        self.conv5 = Conv2d(384, 256, kernel_size=3, padding=1)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = Linear(256 * 6 * 6, 4096)\n",
        "        self.fc2 = Linear(4096, 4096)\n",
        "        self.fc3 = Linear(4096, num_classes)\n",
        "        \n",
        "        # Activation and regularization\n",
        "        self.relu = ReLU()\n",
        "        self.dropout = Dropout(p=0.5)\n",
        "        self.maxpool = MaxPool2d(kernel_size=3, stride=2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through AlexNet.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 3, 224, 224)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # Conv layers with ReLU and pooling\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.relu(self.conv5(x))\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        # Flatten\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.reshape(batch_size, -1)\n",
        "        \n",
        "        # FC layers with ReLU and dropout\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x, training=True)\n",
        "        \n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x, training=True)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test AlexNet\n",
        "res = test_alexnet_architecture(AlexNet)\n",
        "show_result(\"Exercise 1 ‚Äì AlexNet Architecture\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Skip Connections and ResNet\n",
        "\n",
        "### The Vanishing Gradient Problem\n",
        "As networks get deeper, gradients can vanish during backpropagation, making training difficult.\n",
        "\n",
        "### Skip Connections (Residual Connections)\n",
        "ResNet's key innovation: add the input directly to the output of a layer block.\n",
        "\n",
        "```\n",
        "x ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                  ‚îÇ\n",
        "Conv ‚Üí BN ‚Üí ReLU  ‚îÇ\n",
        "‚îÇ                  ‚îÇ\n",
        "Conv ‚Üí BN         ‚îÇ\n",
        "‚îÇ                  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ + ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "       ReLU\n",
        "         ‚îÇ\n",
        "      output\n",
        "```\n",
        "\n",
        "**Mathematical formulation:**\n",
        "- Without skip: $y = F(x)$\n",
        "- With skip: $y = F(x) + x$\n",
        "\n",
        "### Why Skip Connections Work\n",
        "1. **Gradient flow**: Gradients can flow directly through the skip connection\n",
        "2. **Identity mapping**: Network can learn identity function easily (just set F(x) ‚âà 0)\n",
        "3. **Ensemble effect**: Multiple paths through the network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2: Implement ResNet Residual Block (SOLUTION)\n",
        "\n",
        "class ResidualBlock:\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        \"\"\"\n",
        "        Initialize a residual block.\n",
        "        \n",
        "        Args:\n",
        "            in_channels: Number of input channels\n",
        "            out_channels: Number of output channels\n",
        "            stride: Stride for the first convolution\n",
        "        \"\"\"\n",
        "        # Main path (F(x))\n",
        "        self.conv1 = Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = BatchNorm2d(out_channels)\n",
        "        self.conv2 = Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = BatchNorm2d(out_channels)\n",
        "        self.relu = ReLU()\n",
        "        \n",
        "        # Skip connection (downsample if dimensions change)\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0)\n",
        "            self.downsample_bn = BatchNorm2d(out_channels)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the residual block.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, in_channels, H, W)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, out_channels, H', W')\n",
        "        \"\"\"\n",
        "        # Save input for skip connection\n",
        "        identity = x\n",
        "        \n",
        "        # Main path\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out, training=True)\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out, training=True)\n",
        "        \n",
        "        # Apply downsample to identity if needed\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "            identity = self.downsample_bn(identity, training=True)\n",
        "        \n",
        "        # Add skip connection\n",
        "        out = out + identity\n",
        "        \n",
        "        # Apply final ReLU\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Residual Block\n",
        "res = test_resnet_skip_connection(ResidualBlock)\n",
        "show_result(\"Exercise 2 ‚Äì ResNet Skip Connection\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo: ResNet on MNIST\n",
        "\n",
        "Let's see how skip connections help training deeper networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple ResNet for MNIST (28x28 grayscale images)\n",
        "class SimpleResNet:\n",
        "    def __init__(self, num_classes=10):\n",
        "        \"\"\"\n",
        "        A simple ResNet for MNIST classification.\n",
        "        Uses your ResidualBlock implementation.\n",
        "        \"\"\"\n",
        "        self.conv1 = Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = BatchNorm2d(64)\n",
        "        self.relu = ReLU()\n",
        "        \n",
        "        # Residual blocks\n",
        "        self.layer1 = ResidualBlock(64, 64)\n",
        "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
        "        \n",
        "        # Final classification\n",
        "        self.avgpool = MaxPool2d(kernel_size=7)  # Global average pooling\n",
        "        self.fc = Linear(256, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Initial conv\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        \n",
        "        # Residual blocks\n",
        "        x = self.layer1.forward(x)\n",
        "        x = self.layer2.forward(x)\n",
        "        x = self.layer3.forward(x)\n",
        "        \n",
        "        # Classification head\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Generate some dummy MNIST-like data\n",
        "print(\"Generating synthetic MNIST-like data...\")\n",
        "X, y = generate_image_data(n_samples=100, img_size=28, n_channels=1, n_classes=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "print(f\"\\nNote: In a real scenario, you would train this network with gradient descent.\")\n",
        "print(\"For this demo, we just verify the architecture works.\")\n",
        "\n",
        "model = SimpleResNet(num_classes=10)\n",
        "output = model.forward(X_train[:4])  # Forward pass on 4 samples\n",
        "print(f\"\\n‚úì ResNet forward pass successful!\")\n",
        "print(f\"  Input shape: (4, 1, 28, 28)\")\n",
        "print(f\"  Output shape: {output.shape} (batch_size=4, num_classes=10)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LSTM for Time Series\n",
        "\n",
        "### Why RNNs?\n",
        "- Standard neural networks assume independence between inputs\n",
        "- Sequences have temporal dependencies: $x_t$ depends on $x_{t-1}, x_{t-2}, ...$\n",
        "- RNNs maintain a hidden state that captures information from previous time steps\n",
        "\n",
        "### Vanilla RNN Problem\n",
        "Simple RNNs suffer from vanishing/exploding gradients over long sequences.\n",
        "\n",
        "### LSTM: Long Short-Term Memory\n",
        "LSTM solves this with a gating mechanism:\n",
        "\n",
        "1. **Forget gate** ($f_t$): What to forget from cell state\n",
        "2. **Input gate** ($i_t$): What new information to add\n",
        "3. **Output gate** ($o_t$): What to output\n",
        "\n",
        "**LSTM equations:**\n",
        "```\n",
        "f_t = œÉ(W_f ¬∑ [h_{t-1}, x_t] + b_f)      # Forget gate\n",
        "i_t = œÉ(W_i ¬∑ [h_{t-1}, x_t] + b_i)      # Input gate  \n",
        "g_t = tanh(W_g ¬∑ [h_{t-1}, x_t] + b_g)   # Candidate values\n",
        "o_t = œÉ(W_o ¬∑ [h_{t-1}, x_t] + b_o)      # Output gate\n",
        "\n",
        "c_t = f_t ‚äô c_{t-1} + i_t ‚äô g_t          # Update cell state\n",
        "h_t = o_t ‚äô tanh(c_t)                    # Update hidden state\n",
        "```\n",
        "\n",
        "where œÉ is sigmoid, ‚äô is element-wise multiplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3: Implement LSTM Model for Time Series Prediction (SOLUTION)\n",
        "\n",
        "class LSTMModel:\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        \"\"\"\n",
        "        Initialize LSTM model for time series prediction.\n",
        "        \n",
        "        Args:\n",
        "            input_size: Number of input features per time step\n",
        "            hidden_size: Number of hidden units\n",
        "            num_layers: Number of LSTM layers\n",
        "            output_size: Number of output features\n",
        "        \"\"\"\n",
        "        # Initialize LSTM and output layer\n",
        "        self.lstm = LSTM(input_size, hidden_size, num_layers)\n",
        "        self.fc = Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through LSTM.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        # Pass through LSTM\n",
        "        output, (h_n, c_n) = self.lstm.forward(x)\n",
        "        \n",
        "        # Take the last time step\n",
        "        last_output = output[:, -1, :]\n",
        "        \n",
        "        # Pass through FC layer\n",
        "        prediction = self.fc(last_output)\n",
        "        \n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test LSTM\n",
        "res = test_lstm_forward(LSTMModel)\n",
        "show_result(\"Exercise 3 ‚Äì LSTM Forward Pass\", res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: LSTM for Time Series Prediction\n",
        "print(\"Generating synthetic time series data...\")\n",
        "X_ts, y_ts = generate_time_series_data(n_samples=200, seq_len=50, n_features=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ts, y_ts, test_size=0.2)\n",
        "\n",
        "print(f\"Train set: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
        "print(f\"\\nTask: Predict future values from historical sequence\")\n",
        "\n",
        "model = LSTMModel(input_size=1, hidden_size=32, num_layers=2, output_size=1)\n",
        "predictions = model.forward(X_train[:5])\n",
        "print(f\"\\n‚úì LSTM prediction successful!\")\n",
        "print(f\"  Input shape: {X_train[:5].shape}\")\n",
        "print(f\"  Output shape: {predictions.shape}\")\n",
        "print(f\"\\nSample predictions vs actual:\")\n",
        "for i in range(min(3, len(predictions))):\n",
        "    print(f\"  Sample {i}: pred={predictions[i][0]:.3f}, actual={y_train[i][0]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transformers and Attention\n",
        "\n",
        "### Motivation\n",
        "- RNNs process sequences sequentially ‚Üí slow, can't parallelize\n",
        "- Long-range dependencies still challenging despite LSTM\n",
        "- **Solution**: Attention mechanisms\n",
        "\n",
        "### Attention Mechanism\n",
        "**Core idea**: For each position, compute a weighted sum over all positions.\n",
        "\n",
        "**Intuition**: When reading \"The cat sat on the mat\", to understand \"sat\", we should attend to \"cat\" (subject) and \"mat\" (object).\n",
        "\n",
        "### Scaled Dot-Product Attention\n",
        "\n",
        "**Inputs:**\n",
        "- Query (Q): What am I looking for?\n",
        "- Key (K): What do I contain?\n",
        "- Value (V): What do I actually store?\n",
        "\n",
        "**Formula:**\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "**Steps:**\n",
        "1. Compute similarity: $QK^T$ (dot product)\n",
        "2. Scale by $\\sqrt{d_k}$ to prevent large values\n",
        "3. Apply softmax to get attention weights\n",
        "4. Weighted sum of values: multiply by $V$\n",
        "\n",
        "### Multi-Head Attention\n",
        "- Run attention multiple times in parallel with different learned projections\n",
        "- Allows attending to different aspects (e.g., syntactic vs semantic)\n",
        "- Concatenate outputs and project again\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4: Implement Scaled Dot-Product Attention (SOLUTION)\n",
        "\n",
        "def student_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Implement scaled dot-product attention.\n",
        "    \n",
        "    Args:\n",
        "        Q: Query matrix of shape (batch_size, seq_len, d_k)\n",
        "        K: Key matrix of shape (batch_size, seq_len, d_k)\n",
        "        V: Value matrix of shape (batch_size, seq_len, d_k)\n",
        "        mask: Optional mask of shape (seq_len, seq_len)\n",
        "    \n",
        "    Returns:\n",
        "        output: Attention output of shape (batch_size, seq_len, d_k)\n",
        "        attention_weights: Attention weights of shape (batch_size, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    # 1. Get d_k from the last dimension of Q\n",
        "    d_k = Q.shape[-1]\n",
        "    \n",
        "    # 2. Compute scores: Q @ K^T / sqrt(d_k)\n",
        "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
        "    \n",
        "    # 3. Apply mask if provided\n",
        "    if mask is not None:\n",
        "        scores = scores + (mask * -1e9)\n",
        "    \n",
        "    # 4. Apply softmax along the last dimension\n",
        "    attention_weights = softmax(scores, axis=-1)\n",
        "    \n",
        "    # 5. Compute output: attention_weights @ V\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    \n",
        "    # 6. Return output and attention_weights\n",
        "    return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Attention\n",
        "res = test_transformer_attention(student_attention)\n",
        "show_result(\"Exercise 4 ‚Äì Scaled Dot-Product Attention\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Attention Weights\n",
        "\n",
        "Let's visualize what attention learns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Attention Visualization\n",
        "print(\"Creating sample sequence for attention demo...\\n\")\n",
        "\n",
        "# Simple example: 4 words, 8-dimensional embeddings\n",
        "seq_len = 4\n",
        "d_model = 8\n",
        "batch_size = 1\n",
        "\n",
        "# Create simple embeddings (in practice, these would be learned)\n",
        "np.random.seed(42)\n",
        "Q = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)\n",
        "K = Q.copy()  # Self-attention: keys are same as queries\n",
        "V = Q.copy()  # Values are also the same\n",
        "\n",
        "# Compute attention\n",
        "output, weights = student_attention(Q, K, V)\n",
        "\n",
        "print(\"Attention Weights Matrix:\")\n",
        "print(\"(Each row shows how much position i attends to all positions)\\n\")\n",
        "print(\"      Pos0   Pos1   Pos2   Pos3\")\n",
        "for i in range(seq_len):\n",
        "    row_str = f\"Pos{i}: \"\n",
        "    for j in range(seq_len):\n",
        "        row_str += f\"{weights[0, i, j]:.3f}  \"\n",
        "    print(row_str)\n",
        "\n",
        "print(\"\\nNote: Each row sums to 1.0 (softmax normalization)\")\n",
        "print(\"Higher values = stronger attention\")\n",
        "\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "print(\"Output is a weighted combination of all value vectors.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Encoder Block\n",
        "\n",
        "A complete transformer encoder block consists of:\n",
        "1. Multi-head self-attention\n",
        "2. Add & Norm (residual connection + layer normalization)\n",
        "3. Feed-forward network (two linear layers with ReLU)\n",
        "4. Add & Norm again\n",
        "\n",
        "```\n",
        "Input\n",
        "  ‚Üì\n",
        "Multi-Head Attention\n",
        "  ‚Üì\n",
        "Add & Norm (+ residual)\n",
        "  ‚Üì\n",
        "Feed Forward (FFN)\n",
        "  ‚Üì\n",
        "Add & Norm (+ residual)\n",
        "  ‚Üì\n",
        "Output\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Complete Transformer Encoder (provided code)\n",
        "\n",
        "class TransformerEncoder:\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize Transformer Encoder.\n",
        "        \n",
        "        Args:\n",
        "            d_model: Model dimension\n",
        "            num_heads: Number of attention heads\n",
        "            d_ff: Feed-forward dimension\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn_1 = Linear(d_model, d_ff)\n",
        "        self.ffn_2 = Linear(d_ff, d_model)\n",
        "        self.relu = ReLU()\n",
        "        self.dropout = Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass through transformer encoder.\n",
        "        \n",
        "        Args:\n",
        "            x: Input of shape (batch_size, seq_len, d_model)\n",
        "        \n",
        "        Returns:\n",
        "            Output of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Multi-head self-attention with residual\n",
        "        attn_output = self.attention.forward(x, x, x, mask)\n",
        "        x = x + self.dropout(attn_output)  # Residual connection\n",
        "        # In practice, we'd add layer normalization here\n",
        "        \n",
        "        # Feed-forward network with residual\n",
        "        ffn_output = self.ffn_2(self.relu(self.ffn_1(x)))\n",
        "        x = x + self.dropout(ffn_output)  # Residual connection\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Test the encoder\n",
        "print(\"Testing Transformer Encoder...\\n\")\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 256\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "\n",
        "encoder = TransformerEncoder(d_model, num_heads, d_ff)\n",
        "x = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)\n",
        "output = encoder.forward(x)\n",
        "\n",
        "print(f\"‚úì Transformer Encoder working!\")\n",
        "print(f\"  Input shape:  {x.shape}\")\n",
        "print(f\"  Output shape: {output.shape}\")\n",
        "print(f\"\\nKey components:\")\n",
        "print(f\"  - Multi-head attention: {num_heads} heads\")\n",
        "print(f\"  - Model dimension: {d_model}\")\n",
        "print(f\"  - Feed-forward dimension: {d_ff}\")\n",
        "print(f\"  - Two residual connections (attention + FFN)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Positional Encoding\n",
        "print(\"Demonstrating Positional Encoding...\\n\")\n",
        "\n",
        "d_model = 64\n",
        "max_len = 100\n",
        "pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "# Create sample embeddings\n",
        "seq_len = 20\n",
        "batch_size = 1\n",
        "embeddings = np.random.randn(batch_size, seq_len, d_model).astype(np.float32) * 0.1\n",
        "\n",
        "# Add positional encoding\n",
        "embeddings_with_pos = pos_encoder.forward(embeddings)\n",
        "\n",
        "print(f\"Original embeddings shape: {embeddings.shape}\")\n",
        "print(f\"With positional encoding: {embeddings_with_pos.shape}\")\n",
        "print(f\"\\nPositional encoding allows the model to use position information!\")\n",
        "print(f\"Without it, 'cat sat mat' = 'mat cat sat' = 'sat mat cat'\")\n",
        "\n",
        "# Show a few positional encoding values\n",
        "print(f\"\\nSample positional encodings (first 3 positions, first 8 dims):\")\n",
        "for pos in range(3):\n",
        "    print(f\"Position {pos}: {pos_encoder.pe[pos, :8]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Vision Transformers (ViT)\n",
        "\n",
        "### Can Transformers Replace CNNs?\n",
        "In 2020, Vision Transformer (ViT) showed: **YES!** (with enough data)\n",
        "\n",
        "### ViT Architecture\n",
        "\n",
        "**Key idea**: Treat an image as a sequence of patches.\n",
        "\n",
        "```\n",
        "Image (224√ó224√ó3)\n",
        "    ‚Üì\n",
        "Split into patches (16√ó16) ‚Üí 196 patches\n",
        "    ‚Üì\n",
        "Flatten each patch ‚Üí 196 vectors of size 768\n",
        "    ‚Üì\n",
        "Linear projection (patch embedding)\n",
        "    ‚Üì\n",
        "Add [CLS] token + positional encoding\n",
        "    ‚Üì\n",
        "Transformer Encoder (12-24 layers)\n",
        "    ‚Üì\n",
        "[CLS] token ‚Üí Classification Head\n",
        "```\n",
        "\n",
        "### ViT vs CNN\n",
        "\n",
        "**CNN advantages:**\n",
        "- Strong inductive biases (locality, translation invariance)\n",
        "- Works well with less data\n",
        "- More efficient for small images\n",
        "\n",
        "**ViT advantages:**\n",
        "- Global receptive field from layer 1\n",
        "- More flexible (no hardcoded filters)\n",
        "- Scales better with data and compute\n",
        "- Better for very large images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 5: Implement Patch Embedding for ViT (SOLUTION)\n",
        "\n",
        "class PatchEmbedding:\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, d_model=768):\n",
        "        \"\"\"\n",
        "        Initialize patch embedding layer.\n",
        "        \n",
        "        Args:\n",
        "            img_size: Input image size (assumes square images)\n",
        "            patch_size: Size of each patch (assumes square patches)\n",
        "            in_channels: Number of input channels (3 for RGB)\n",
        "            d_model: Embedding dimension\n",
        "        \"\"\"\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        # Create a linear projection for patches\n",
        "        self.patch_dim = in_channels * patch_size * patch_size\n",
        "        self.proj = Linear(self.patch_dim, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Convert image to patch embeddings.\n",
        "        \n",
        "        Args:\n",
        "            x: Input image of shape (batch_size, in_channels, img_size, img_size)\n",
        "        \n",
        "        Returns:\n",
        "            Patch embeddings of shape (batch_size, n_patches, d_model)\n",
        "        \"\"\"\n",
        "        batch_size, in_channels, H, W = x.shape\n",
        "        \n",
        "        # Calculate number of patches\n",
        "        n_patches_h = H // self.patch_size\n",
        "        n_patches_w = W // self.patch_size\n",
        "        \n",
        "        # Extract patches\n",
        "        patches = []\n",
        "        for i in range(n_patches_h):\n",
        "            for j in range(n_patches_w):\n",
        "                # Extract patch\n",
        "                patch = x[:, :, \n",
        "                         i*self.patch_size:(i+1)*self.patch_size,\n",
        "                         j*self.patch_size:(j+1)*self.patch_size]\n",
        "                # Flatten patch\n",
        "                patch_flat = patch.reshape(batch_size, -1)\n",
        "                patches.append(patch_flat)\n",
        "        \n",
        "        # Stack patches\n",
        "        patches = np.stack(patches, axis=1)  # (batch_size, n_patches, patch_dim)\n",
        "        \n",
        "        # Apply linear projection\n",
        "        patch_embeddings = self.proj(patches)\n",
        "        \n",
        "        return patch_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Patch Embedding\n",
        "res = test_vit_patch_embedding(PatchEmbedding)\n",
        "show_result(\"Exercise 5 ‚Äì ViT Patch Embedding\", res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Complete ViT Forward Pass\n",
        "print(\"Demonstrating Vision Transformer (ViT)...\\n\")\n",
        "\n",
        "class VisionTransformer:\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, \n",
        "                 num_classes=1000, d_model=768, num_heads=12, num_layers=12, d_ff=3072):\n",
        "        \"\"\"\n",
        "        Complete Vision Transformer implementation.\n",
        "        \"\"\"\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, d_model)\n",
        "        self.n_patches = self.patch_embed.n_patches\n",
        "        \n",
        "        # CLS token (learnable parameter)\n",
        "        self.cls_token = np.random.randn(1, 1, d_model).astype(np.float32) * 0.02\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=self.n_patches + 1)\n",
        "        \n",
        "        # Transformer encoder layers\n",
        "        self.encoders = [TransformerEncoder(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
        "        \n",
        "        # Classification head\n",
        "        self.head = Linear(d_model, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through ViT.\n",
        "        \n",
        "        Args:\n",
        "            x: Input images of shape (batch_size, in_channels, img_size, img_size)\n",
        "        \n",
        "        Returns:\n",
        "            Class logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        # 1. Patch embedding\n",
        "        x = self.patch_embed.forward(x)  # (B, n_patches, d_model)\n",
        "        \n",
        "        # 2. Prepend CLS token\n",
        "        cls_tokens = np.repeat(self.cls_token, batch_size, axis=0)\n",
        "        x = np.concatenate([cls_tokens, x], axis=1)  # (B, n_patches+1, d_model)\n",
        "        \n",
        "        # 3. Add positional encoding\n",
        "        x = self.pos_encoding.forward(x)\n",
        "        \n",
        "        # 4. Pass through transformer encoders\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder.forward(x)\n",
        "        \n",
        "        # 5. Classification using CLS token\n",
        "        cls_output = x[:, 0, :]  # Take CLS token\n",
        "        logits = self.head(cls_output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# Create a small ViT for demonstration\n",
        "vit = VisionTransformer(\n",
        "    img_size=224, \n",
        "    patch_size=16, \n",
        "    num_classes=10,\n",
        "    d_model=192,  # Smaller for demo\n",
        "    num_heads=3,\n",
        "    num_layers=6,  # Fewer layers for demo\n",
        "    d_ff=768\n",
        ")\n",
        "\n",
        "# Test forward pass\n",
        "test_img = np.random.randn(2, 3, 224, 224).astype(np.float32)\n",
        "output = vit.forward(test_img)\n",
        "\n",
        "print(f\"‚úì Vision Transformer working!\")\n",
        "print(f\"\\nArchitecture:\")\n",
        "print(f\"  - Image size: 224√ó224\")\n",
        "print(f\"  - Patch size: 16√ó16\")\n",
        "print(f\"  - Number of patches: {vit.n_patches}\")\n",
        "print(f\"  - Embedding dimension: 192\")\n",
        "print(f\"  - Attention heads: 3\")\n",
        "print(f\"  - Transformer layers: 6\")\n",
        "print(f\"\\nForward pass:\")\n",
        "print(f\"  - Input: {test_img.shape}\")\n",
        "print(f\"  - Output: {output.shape}\")\n",
        "print(f\"\\nViT treats images as sequences of patches!\")\n",
        "print(f\"No convolutions needed ‚Äì pure transformer architecture.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Comparison\n",
        "\n",
        "### Architecture Comparison\n",
        "\n",
        "| Architecture | Best For | Key Innovation | Parameters (typical) |\n",
        "|--------------|----------|----------------|---------------------|\n",
        "| **AlexNet** | Image classification | Deep CNNs, ReLU, Dropout | ~60M |\n",
        "| **ResNet** | Very deep networks | Skip connections | 25M-60M |\n",
        "| **LSTM** | Sequential data | Gating mechanism | Varies |\n",
        "| **Transformer** | Long sequences | Attention, parallelization | 100M-1B+ |\n",
        "| **ViT** | Images (with lots of data) | Patch-based transformers | 86M-632M |\n",
        "\n",
        "### When to Use What?\n",
        "\n",
        "**Use CNNs (AlexNet/ResNet) when:**\n",
        "- Working with images\n",
        "- Limited training data\n",
        "- Need translation invariance\n",
        "- Want efficiency\n",
        "\n",
        "**Use RNNs/LSTMs when:**\n",
        "- Sequential data with temporal dependencies\n",
        "- Online/streaming processing\n",
        "- Audio, time series, text (small scale)\n",
        "\n",
        "**Use Transformers when:**\n",
        "- Need long-range dependencies\n",
        "- Have lots of data and compute\n",
        "- Want parallelization\n",
        "- NLP tasks, large-scale vision\n",
        "\n",
        "### Modern Trends (2024)\n",
        "1. **Hybrid architectures**: Combining CNNs + Transformers (e.g., ConvNeXt)\n",
        "2. **Efficient transformers**: Reducing computational cost\n",
        "3. **Vision-language models**: CLIP, Flamingo (multimodal)\n",
        "4. **Foundation models**: Pre-trained on massive data, fine-tuned for tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Reflection Questions\n",
        "\n",
        "1. **Skip connections**: Why do skip connections help with training very deep networks? What problem do they solve?\n",
        "\n",
        "2. **LSTM vs Transformer**: Both handle sequential data. What are the key differences in how they process sequences? When would you choose one over the other?\n",
        "\n",
        "3. **Attention mechanism**: Explain in your own words how scaled dot-product attention works. Why is the scaling factor $\\sqrt{d_k}$ important?\n",
        "\n",
        "4. **ViT vs CNN**: Vision Transformers treat images as sequences of patches, while CNNs use convolutions. What are the trade-offs? Why does ViT need more data than CNNs?\n",
        "\n",
        "5. **Inductive biases**: CNNs have strong inductive biases (locality, translation invariance), while Transformers have fewer. What does this mean for learning and generalization?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answers:**\n",
        "\n",
        "**1) Skip connections** allow gradients to flow directly through the network via the identity path, solving the vanishing gradient problem. They enable training very deep networks (100+ layers) by providing a direct path for gradient flow. The network can easily learn the identity function (by setting F(x) ‚âà 0), making it easier to add depth without hurting performance.\n",
        "\n",
        "**2) Key differences:**\n",
        "- **LSTM**: Processes sequentially (one token at a time), maintains hidden state, can't parallelize across sequence length, better for streaming/online tasks\n",
        "- **Transformer**: Processes all positions simultaneously using attention, fully parallelizable, better for long-range dependencies, requires more data\n",
        "\n",
        "Choose LSTM for: streaming data, limited compute, small datasets\n",
        "Choose Transformer for: parallel processing, long sequences, abundant data\n",
        "\n",
        "**3) Scaled dot-product attention** computes similarity between queries and keys (via dot product), then uses these similarities as weights to combine values. The scaling factor $1/\\sqrt{d_k}$ prevents dot products from becoming too large (which would push softmax into regions with tiny gradients), keeping training stable.\n",
        "\n",
        "**4) Trade-offs:**\n",
        "- **CNNs**: Built-in locality and translation invariance ‚Üí sample efficient, work with less data\n",
        "- **ViT**: No built-in assumptions ‚Üí need to learn everything from data, require massive datasets (millions of images) but can achieve better performance with sufficient data and compute. ViT has global receptive field from layer 1, while CNNs build it up gradually.\n",
        "\n",
        "**5) Inductive biases** are built-in assumptions about the data structure. CNNs assume nearby pixels are related (locality) and that patterns repeat across the image (translation invariance). This makes learning easier with limited data. Transformers make fewer assumptions, making them more flexible but requiring more data to learn these patterns. Stronger inductive biases ‚Üí better generalization with less data, but potentially limited by assumptions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Next Steps\n",
        "\n",
        "### For Your Projects\n",
        "1. **Start simple**: Use pre-trained models (transfer learning)\n",
        "2. **Image tasks**: Try ResNet or ViT from PyTorch/TensorFlow\n",
        "3. **Sequence tasks**: Use transformers (Hugging Face library)\n",
        "4. **Don't reinvent**: Leverage existing implementations\n",
        "\n",
        "### Further Learning\n",
        "- **Papers**: Original papers (AlexNet, ResNet, Attention is All You Need, ViT)\n",
        "- **Courses**: CS231n (Stanford), CS224n (Stanford)\n",
        "- **Implementations**: PyTorch tutorials, TensorFlow guides\n",
        "- **Practice**: Kaggle competitions, personal projects\n",
        "\n",
        "**Congratulations!** You've now seen the major architectures powering modern AI. üéâ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
