{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 8 ‚Äî Deep Learning Architectures\n",
        "\n",
        "## Objectives\n",
        "- Understand the evolution of deep learning architectures\n",
        "- Implement AlexNet for image classification\n",
        "- Learn about skip connections and build ResNet blocks\n",
        "- Implement LSTM for time series prediction\n",
        "- Understand the Transformer architecture and attention mechanism\n",
        "- Explore Vision Transformers (ViT)\n",
        "\n",
        "This notebook provides a comprehensive tour through the major deep learning architectures that have shaped modern AI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from utils import (\n",
        "    show_result, generate_image_data, generate_time_series_data,\n",
        "    generate_sequence_classification_data, train_test_split,\n",
        "    Conv2d, MaxPool2d, ReLU, Dropout, Linear, BatchNorm2d,\n",
        "    LSTM, LSTMCell, MultiHeadAttention, PositionalEncoding,\n",
        "    scaled_dot_product_attention, softmax, accuracy, mse,\n",
        "    test_alexnet_architecture, test_resnet_skip_connection,\n",
        "    test_lstm_forward, test_transformer_attention, test_vit_patch_embedding\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Deep Learning Architectures\n",
        "\n",
        "### Brief History\n",
        "- **2012**: AlexNet wins ImageNet, sparking the deep learning revolution\n",
        "- **2015**: ResNet introduces skip connections, enabling very deep networks (152+ layers)\n",
        "- **2017**: Transformers revolutionize NLP with attention mechanisms\n",
        "- **2020**: Vision Transformers (ViT) show transformers can excel at computer vision\n",
        "\n",
        "### Key Architecture Families\n",
        "\n",
        "1. **Convolutional Neural Networks (CNNs)**\n",
        "   - Designed for spatial data (images)\n",
        "   - Use local connectivity and weight sharing\n",
        "   - Examples: LeNet, AlexNet, VGG, ResNet, Inception\n",
        "\n",
        "2. **Recurrent Neural Networks (RNNs)**\n",
        "   - Designed for sequential data (text, time series)\n",
        "   - Maintain hidden state across time steps\n",
        "   - Examples: Vanilla RNN, LSTM, GRU\n",
        "\n",
        "3. **Transformers**\n",
        "   - Use attention mechanisms to process sequences\n",
        "   - Can be parallelized (unlike RNNs)\n",
        "   - Examples: BERT, GPT, T5, ViT\n",
        "\n",
        "### Why Different Architectures?\n",
        "- **Inductive biases**: Built-in assumptions about the data structure\n",
        "- **CNNs** assume spatial locality and translation invariance\n",
        "- **RNNs** assume sequential dependencies\n",
        "- **Transformers** make fewer assumptions, learn patterns from data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. AlexNet: The CNN Revolution\n",
        "\n",
        "AlexNet (2012) was the breakthrough that brought deep learning to mainstream computer vision.\n",
        "\n",
        "### Architecture Overview\n",
        "```\n",
        "Input (224√ó224√ó3)\n",
        "    ‚Üì\n",
        "Conv1 (11√ó11, stride 4) ‚Üí 96 filters ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Conv2 (5√ó5) ‚Üí 256 filters ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Conv3 (3√ó3) ‚Üí 384 filters ‚Üí ReLU\n",
        "    ‚Üì\n",
        "Conv4 (3√ó3) ‚Üí 384 filters ‚Üí ReLU\n",
        "    ‚Üì\n",
        "Conv5 (3√ó3) ‚Üí 256 filters ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Flatten\n",
        "    ‚Üì\n",
        "FC1 (4096) ‚Üí ReLU ‚Üí Dropout\n",
        "    ‚Üì\n",
        "FC2 (4096) ‚Üí ReLU ‚Üí Dropout\n",
        "    ‚Üì\n",
        "FC3 (num_classes)\n",
        "```\n",
        "\n",
        "### Key Innovations\n",
        "1. **ReLU activation**: Faster training than sigmoid/tanh\n",
        "2. **Dropout**: Regularization to prevent overfitting\n",
        "3. **Data augmentation**: Random crops, flips\n",
        "4. **GPU training**: Made deep networks practical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Implement AlexNet\n",
        "\n",
        "class AlexNet:\n",
        "    def __init__(self, num_classes=10):\n",
        "        \"\"\"\n",
        "        Initialize AlexNet architecture.\n",
        "        For simplicity, we'll use a slightly smaller version.\n",
        "        \n",
        "        Args:\n",
        "            num_classes: Number of output classes\n",
        "        \"\"\"\n",
        "        # TODO: Define convolutional layers\n",
        "        # Hint: Use Conv2d, ReLU, MaxPool2d from utils\n",
        "        # self.conv1 = Conv2d(3, 96, kernel_size=11, stride=4, padding=2)\n",
        "        # self.conv2 = Conv2d(96, 256, kernel_size=5, padding=2)\n",
        "        # ... continue for conv3, conv4, conv5\n",
        "        \n",
        "        raise NotImplementedError\n",
        "        \n",
        "        # TODO: Define fully connected layers\n",
        "        # self.fc1 = Linear(256 * 6 * 6, 4096)  # Adjust input size based on your conv layers\n",
        "        # self.fc2 = Linear(4096, 4096)\n",
        "        # self.fc3 = Linear(4096, num_classes)\n",
        "        \n",
        "        # TODO: Define activation and regularization\n",
        "        # self.relu = ReLU()\n",
        "        # self.dropout = Dropout(p=0.5)\n",
        "        # self.maxpool = MaxPool2d(kernel_size=3, stride=2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through AlexNet.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 3, 224, 224)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass\n",
        "        # 1. Pass through conv layers with ReLU and pooling\n",
        "        # 2. Flatten the output\n",
        "        # 3. Pass through FC layers with ReLU and dropout\n",
        "        # 4. Return final output\n",
        "        \n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test AlexNet\n",
        "res = test_alexnet_architecture(AlexNet)\n",
        "show_result(\"Exercise 1 ‚Äì AlexNet Architecture\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Skip Connections and ResNet\n",
        "\n",
        "### The Vanishing Gradient Problem\n",
        "As networks get deeper, gradients can vanish during backpropagation, making training difficult.\n",
        "\n",
        "### Skip Connections (Residual Connections)\n",
        "ResNet's key innovation: add the input directly to the output of a layer block.\n",
        "\n",
        "```\n",
        "x ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                  ‚îÇ\n",
        "Conv ‚Üí BN ‚Üí ReLU  ‚îÇ\n",
        "‚îÇ                  ‚îÇ\n",
        "Conv ‚Üí BN         ‚îÇ\n",
        "‚îÇ                  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ + ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "       ReLU\n",
        "         ‚îÇ\n",
        "      output\n",
        "```\n",
        "\n",
        "**Mathematical formulation:**\n",
        "- Without skip: $y = F(x)$\n",
        "- With skip: $y = F(x) + x$\n",
        "\n",
        "### Why Skip Connections Work\n",
        "1. **Gradient flow**: Gradients can flow directly through the skip connection\n",
        "2. **Identity mapping**: Network can learn identity function easily (just set F(x) ‚âà 0)\n",
        "3. **Ensemble effect**: Multiple paths through the network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2: Implement ResNet Residual Block\n",
        "\n",
        "class ResidualBlock:\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        \"\"\"\n",
        "        Initialize a residual block.\n",
        "        \n",
        "        Args:\n",
        "            in_channels: Number of input channels\n",
        "            out_channels: Number of output channels\n",
        "            stride: Stride for the first convolution\n",
        "        \"\"\"\n",
        "        # TODO: Define the main path (F(x))\n",
        "        # self.conv1 = Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        # self.bn1 = BatchNorm2d(out_channels)\n",
        "        # self.conv2 = Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        # self.bn2 = BatchNorm2d(out_channels)\n",
        "        # self.relu = ReLU()\n",
        "        \n",
        "        raise NotImplementedError\n",
        "        \n",
        "        # TODO: Define the skip connection\n",
        "        # If dimensions change, use a 1x1 conv to match dimensions\n",
        "        # self.downsample = None\n",
        "        # if stride != 1 or in_channels != out_channels:\n",
        "        #     self.downsample = ...\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the residual block.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, in_channels, H, W)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, out_channels, H', W')\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass with skip connection\n",
        "        # 1. Save input for skip connection: identity = x\n",
        "        # 2. Pass through conv1 ‚Üí bn1 ‚Üí relu\n",
        "        # 3. Pass through conv2 ‚Üí bn2 (no ReLU yet!)\n",
        "        # 4. If needed, apply downsample to identity\n",
        "        # 5. Add: out = out + identity\n",
        "        # 6. Apply final ReLU\n",
        "        \n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Residual Block\n",
        "res = test_resnet_skip_connection(ResidualBlock)\n",
        "show_result(\"Exercise 2 ‚Äì ResNet Skip Connection\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo: ResNet on MNIST\n",
        "\n",
        "Let's see how skip connections help training deeper networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple ResNet for MNIST (28x28 grayscale images)\n",
        "class SimpleResNet:\n",
        "    def __init__(self, num_classes=10):\n",
        "        \"\"\"\n",
        "        A simple ResNet for MNIST classification.\n",
        "        Uses your ResidualBlock implementation.\n",
        "        \"\"\"\n",
        "        self.conv1 = Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = BatchNorm2d(64)\n",
        "        self.relu = ReLU()\n",
        "        \n",
        "        # Residual blocks\n",
        "        self.layer1 = ResidualBlock(64, 64)\n",
        "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
        "        \n",
        "        # Final classification\n",
        "        self.avgpool = MaxPool2d(kernel_size=7)  # Global average pooling\n",
        "        self.fc = Linear(256, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Initial conv\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        \n",
        "        # Residual blocks\n",
        "        x = self.layer1.forward(x)\n",
        "        x = self.layer2.forward(x)\n",
        "        x = self.layer3.forward(x)\n",
        "        \n",
        "        # Classification head\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Generate some dummy MNIST-like data\n",
        "print(\"Generating synthetic MNIST-like data...\")\n",
        "X, y = generate_image_data(n_samples=100, img_size=28, n_channels=1, n_classes=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "print(f\"\\nNote: In a real scenario, you would train this network with gradient descent.\")\n",
        "print(\"For this demo, we just verify the architecture works.\")\n",
        "\n",
        "try:\n",
        "    model = SimpleResNet(num_classes=10)\n",
        "    output = model.forward(X_train[:4])  # Forward pass on 4 samples\n",
        "    print(f\"\\n‚úì ResNet forward pass successful!\")\n",
        "    print(f\"  Input shape: (4, 1, 28, 28)\")\n",
        "    print(f\"  Output shape: {output.shape} (batch_size=4, num_classes=10)\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚úó Error in ResNet: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LSTM for Time Series\n",
        "\n",
        "### Why RNNs?\n",
        "- Standard neural networks assume independence between inputs\n",
        "- Sequences have temporal dependencies: $x_t$ depends on $x_{t-1}, x_{t-2}, ...$\n",
        "- RNNs maintain a hidden state that captures information from previous time steps\n",
        "\n",
        "### Vanilla RNN Problem\n",
        "Simple RNNs suffer from vanishing/exploding gradients over long sequences.\n",
        "\n",
        "### LSTM: Long Short-Term Memory\n",
        "LSTM solves this with a gating mechanism:\n",
        "\n",
        "1. **Forget gate** ($f_t$): What to forget from cell state\n",
        "2. **Input gate** ($i_t$): What new information to add\n",
        "3. **Output gate** ($o_t$): What to output\n",
        "\n",
        "**LSTM equations:**\n",
        "```\n",
        "f_t = œÉ(W_f ¬∑ [h_{t-1}, x_t] + b_f)      # Forget gate\n",
        "i_t = œÉ(W_i ¬∑ [h_{t-1}, x_t] + b_i)      # Input gate  \n",
        "g_t = tanh(W_g ¬∑ [h_{t-1}, x_t] + b_g)   # Candidate values\n",
        "o_t = œÉ(W_o ¬∑ [h_{t-1}, x_t] + b_o)      # Output gate\n",
        "\n",
        "c_t = f_t ‚äô c_{t-1} + i_t ‚äô g_t          # Update cell state\n",
        "h_t = o_t ‚äô tanh(c_t)                    # Update hidden state\n",
        "```\n",
        "\n",
        "where œÉ is sigmoid, ‚äô is element-wise multiplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3: Implement LSTM Model for Time Series Prediction\n",
        "\n",
        "class LSTMModel:\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        \"\"\"\n",
        "        Initialize LSTM model for time series prediction.\n",
        "        \n",
        "        Args:\n",
        "            input_size: Number of input features per time step\n",
        "            hidden_size: Number of hidden units\n",
        "            num_layers: Number of LSTM layers\n",
        "            output_size: Number of output features\n",
        "        \"\"\"\n",
        "        # TODO: Initialize LSTM and output layer\n",
        "        # Hint: Use the LSTM class from utils\n",
        "        # self.lstm = LSTM(input_size, hidden_size, num_layers)\n",
        "        # self.fc = Linear(hidden_size, output_size)\n",
        "        \n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through LSTM.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass\n",
        "        # 1. Pass through LSTM: output, (h_n, c_n) = self.lstm.forward(x)\n",
        "        # 2. Take the last time step: last_output = output[:, -1, :]\n",
        "        # 3. Pass through FC layer: prediction = self.fc(last_output)\n",
        "        # 4. Return prediction\n",
        "        \n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test LSTM\n",
        "res = test_lstm_forward(LSTMModel)\n",
        "show_result(\"Exercise 3 ‚Äì LSTM Forward Pass\", res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: LSTM for Time Series Prediction\n",
        "print(\"Generating synthetic time series data...\")\n",
        "X_ts, y_ts = generate_time_series_data(n_samples=200, seq_len=50, n_features=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ts, y_ts, test_size=0.2)\n",
        "\n",
        "print(f\"Train set: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
        "print(f\"\\nTask: Predict future values from historical sequence\")\n",
        "\n",
        "try:\n",
        "    model = LSTMModel(input_size=1, hidden_size=32, num_layers=2, output_size=1)\n",
        "    predictions = model.forward(X_train[:5])\n",
        "    print(f\"\\n‚úì LSTM prediction successful!\")\n",
        "    print(f\"  Input shape: {X_train[:5].shape}\")\n",
        "    print(f\"  Output shape: {predictions.shape}\")\n",
        "    print(f\"\\nSample predictions vs actual:\")\n",
        "    for i in range(min(3, len(predictions))):\n",
        "        print(f\"  Sample {i}: pred={predictions[i][0]:.3f}, actual={y_train[i][0]:.3f}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚úó Error in LSTM: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transformers and Attention\n",
        "\n",
        "### Motivation\n",
        "- RNNs process sequences sequentially ‚Üí slow, can't parallelize\n",
        "- Long-range dependencies still challenging despite LSTM\n",
        "- **Solution**: Attention mechanisms\n",
        "\n",
        "### Attention Mechanism\n",
        "**Core idea**: For each position, compute a weighted sum over all positions.\n",
        "\n",
        "**Intuition**: When reading \"The cat sat on the mat\", to understand \"sat\", we should attend to \"cat\" (subject) and \"mat\" (object).\n",
        "\n",
        "### Scaled Dot-Product Attention\n",
        "\n",
        "**Inputs:**\n",
        "- Query (Q): What am I looking for?\n",
        "- Key (K): What do I contain?\n",
        "- Value (V): What do I actually store?\n",
        "\n",
        "**Formula:**\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "**Steps:**\n",
        "1. Compute similarity: $QK^T$ (dot product)\n",
        "2. Scale by $\\sqrt{d_k}$ to prevent large values\n",
        "3. Apply softmax to get attention weights\n",
        "4. Weighted sum of values: multiply by $V$\n",
        "\n",
        "### Multi-Head Attention\n",
        "- Run attention multiple times in parallel with different learned projections\n",
        "- Allows attending to different aspects (e.g., syntactic vs semantic)\n",
        "- Concatenate outputs and project again\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4: Implement Scaled Dot-Product Attention\n",
        "\n",
        "def student_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Implement scaled dot-product attention.\n",
        "    \n",
        "    Args:\n",
        "        Q: Query matrix of shape (batch_size, seq_len, d_k)\n",
        "        K: Key matrix of shape (batch_size, seq_len, d_k)\n",
        "        V: Value matrix of shape (batch_size, seq_len, d_k)\n",
        "        mask: Optional mask of shape (seq_len, seq_len)\n",
        "    \n",
        "    Returns:\n",
        "        output: Attention output of shape (batch_size, seq_len, d_k)\n",
        "        attention_weights: Attention weights of shape (batch_size, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    # TODO: Implement scaled dot-product attention\n",
        "    # 1. Get d_k from the last dimension of Q\n",
        "    # 2. Compute scores: Q @ K^T / sqrt(d_k)\n",
        "    #    Hint: Use np.matmul(Q, K.transpose(0, 2, 1)) for batch matrix multiply\n",
        "    # 3. Apply mask if provided: scores = scores + (mask * -1e9)\n",
        "    # 4. Apply softmax along the last dimension\n",
        "    #    Hint: Use softmax from utils\n",
        "    # 5. Compute output: attention_weights @ V\n",
        "    # 6. Return output and attention_weights\n",
        "    \n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Attention\n",
        "res = test_transformer_attention(student_attention)\n",
        "show_result(\"Exercise 4 ‚Äì Scaled Dot-Product Attention\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Attention Weights\n",
        "\n",
        "Let's visualize what attention learns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Attention Visualization\n",
        "print(\"Creating sample sequence for attention demo...\\n\")\n",
        "\n",
        "# Simple example: 4 words, 8-dimensional embeddings\n",
        "seq_len = 4\n",
        "d_model = 8\n",
        "batch_size = 1\n",
        "\n",
        "# Create simple embeddings (in practice, these would be learned)\n",
        "np.random.seed(42)\n",
        "Q = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)\n",
        "K = Q.copy()  # Self-attention: keys are same as queries\n",
        "V = Q.copy()  # Values are also the same\n",
        "\n",
        "# Compute attention\n",
        "try:\n",
        "    output, weights = student_attention(Q, K, V)\n",
        "    \n",
        "    print(\"Attention Weights Matrix:\")\n",
        "    print(\"(Each row shows how much position i attends to all positions)\\n\")\n",
        "    print(\"      Pos0   Pos1   Pos2   Pos3\")\n",
        "    for i in range(seq_len):\n",
        "        row_str = f\"Pos{i}: \"\n",
        "        for j in range(seq_len):\n",
        "            row_str += f\"{weights[0, i, j]:.3f}  \"\n",
        "        print(row_str)\n",
        "    \n",
        "    print(\"\\nNote: Each row sums to 1.0 (softmax normalization)\")\n",
        "    print(\"Higher values = stronger attention\")\n",
        "    \n",
        "    print(f\"\\nOutput shape: {output.shape}\")\n",
        "    print(\"Output is a weighted combination of all value vectors.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Complete Exercise 4 first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Encoder Block\n",
        "\n",
        "A complete transformer encoder block consists of:\n",
        "1. Multi-head self-attention\n",
        "2. Add & Norm (residual connection + layer normalization)\n",
        "3. Feed-forward network (two linear layers with ReLU)\n",
        "4. Add & Norm again\n",
        "\n",
        "```\n",
        "Input\n",
        "  ‚Üì\n",
        "Multi-Head Attention\n",
        "  ‚Üì\n",
        "Add & Norm (+ residual)\n",
        "  ‚Üì\n",
        "Feed Forward (FFN)\n",
        "  ‚Üì\n",
        "Add & Norm (+ residual)\n",
        "  ‚Üì\n",
        "Output\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Complete Transformer Encoder (provided code)\n",
        "\n",
        "class TransformerEncoder:\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize Transformer Encoder.\n",
        "        \n",
        "        Args:\n",
        "            d_model: Model dimension\n",
        "            num_heads: Number of attention heads\n",
        "            d_ff: Feed-forward dimension\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn_1 = Linear(d_model, d_ff)\n",
        "        self.ffn_2 = Linear(d_ff, d_model)\n",
        "        self.relu = ReLU()\n",
        "        self.dropout = Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass through transformer encoder.\n",
        "        \n",
        "        Args:\n",
        "            x: Input of shape (batch_size, seq_len, d_model)\n",
        "        \n",
        "        Returns:\n",
        "            Output of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Multi-head self-attention with residual\n",
        "        attn_output = self.attention.forward(x, x, x, mask)\n",
        "        x = x + self.dropout(attn_output)  # Residual connection\n",
        "        # In practice, we'd add layer normalization here\n",
        "        \n",
        "        # Feed-forward network with residual\n",
        "        ffn_output = self.ffn_2(self.relu(self.ffn_1(x)))\n",
        "        x = x + self.dropout(ffn_output)  # Residual connection\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Test the encoder\n",
        "print(\"Testing Transformer Encoder...\\n\")\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 256\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "\n",
        "encoder = TransformerEncoder(d_model, num_heads, d_ff)\n",
        "x = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)\n",
        "output = encoder.forward(x)\n",
        "\n",
        "print(f\"‚úì Transformer Encoder working!\")\n",
        "print(f\"  Input shape:  {x.shape}\")\n",
        "print(f\"  Output shape: {output.shape}\")\n",
        "print(f\"\\nKey components:\")\n",
        "print(f\"  - Multi-head attention: {num_heads} heads\")\n",
        "print(f\"  - Model dimension: {d_model}\")\n",
        "print(f\"  - Feed-forward dimension: {d_ff}\")\n",
        "print(f\"  - Two residual connections (attention + FFN)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Positional Encoding\n",
        "print(\"Demonstrating Positional Encoding...\\n\")\n",
        "\n",
        "d_model = 64\n",
        "max_len = 100\n",
        "pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "# Create sample embeddings\n",
        "seq_len = 20\n",
        "batch_size = 1\n",
        "embeddings = np.random.randn(batch_size, seq_len, d_model).astype(np.float32) * 0.1\n",
        "\n",
        "# Add positional encoding\n",
        "embeddings_with_pos = pos_encoder.forward(embeddings)\n",
        "\n",
        "print(f\"Original embeddings shape: {embeddings.shape}\")\n",
        "print(f\"With positional encoding: {embeddings_with_pos.shape}\")\n",
        "print(f\"\\nPositional encoding allows the model to use position information!\")\n",
        "print(f\"Without it, 'cat sat mat' = 'mat cat sat' = 'sat mat cat'\")\n",
        "\n",
        "# Show a few positional encoding values\n",
        "print(f\"\\nSample positional encodings (first 3 positions, first 8 dims):\")\n",
        "for pos in range(3):\n",
        "    print(f\"Position {pos}: {pos_encoder.pe[pos, :8]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Vision Transformers (ViT)\n",
        "\n",
        "### Can Transformers Replace CNNs?\n",
        "In 2020, Vision Transformer (ViT) showed: **YES!** (with enough data)\n",
        "\n",
        "### ViT Architecture\n",
        "\n",
        "**Key idea**: Treat an image as a sequence of patches.\n",
        "\n",
        "```\n",
        "Image (224√ó224√ó3)\n",
        "    ‚Üì\n",
        "Split into patches (16√ó16) ‚Üí 196 patches\n",
        "    ‚Üì\n",
        "Flatten each patch ‚Üí 196 vectors of size 768\n",
        "    ‚Üì\n",
        "Linear projection (patch embedding)\n",
        "    ‚Üì\n",
        "Add [CLS] token + positional encoding\n",
        "    ‚Üì\n",
        "Transformer Encoder (12-24 layers)\n",
        "    ‚Üì\n",
        "[CLS] token ‚Üí Classification Head\n",
        "```\n",
        "\n",
        "### ViT vs CNN\n",
        "\n",
        "**CNN advantages:**\n",
        "- Strong inductive biases (locality, translation invariance)\n",
        "- Works well with less data\n",
        "- More efficient for small images\n",
        "\n",
        "**ViT advantages:**\n",
        "- Global receptive field from layer 1\n",
        "- More flexible (no hardcoded filters)\n",
        "- Scales better with data and compute\n",
        "- Better for very large images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 5: Implement Patch Embedding for ViT\n",
        "\n",
        "class PatchEmbedding:\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, d_model=768):\n",
        "        \"\"\"\n",
        "        Initialize patch embedding layer.\n",
        "        \n",
        "        Args:\n",
        "            img_size: Input image size (assumes square images)\n",
        "            patch_size: Size of each patch (assumes square patches)\n",
        "            in_channels: Number of input channels (3 for RGB)\n",
        "            d_model: Embedding dimension\n",
        "        \"\"\"\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        # TODO: Create a linear projection for patches\n",
        "        # Each patch is patch_size √ó patch_size √ó in_channels flattened\n",
        "        # self.patch_dim = in_channels * patch_size * patch_size\n",
        "        # self.proj = Linear(self.patch_dim, d_model)\n",
        "        \n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Convert image to patch embeddings.\n",
        "        \n",
        "        Args:\n",
        "            x: Input image of shape (batch_size, in_channels, img_size, img_size)\n",
        "        \n",
        "        Returns:\n",
        "            Patch embeddings of shape (batch_size, n_patches, d_model)\n",
        "        \"\"\"\n",
        "        # TODO: Implement patch extraction and embedding\n",
        "        # 1. Extract patches from the image\n",
        "        #    For simplicity, reshape: (B, C, H, W) ‚Üí (B, n_patches, patch_dim)\n",
        "        # 2. Apply linear projection to each patch\n",
        "        # 3. Return patch embeddings\n",
        "        \n",
        "        # Hint: You can use np.reshape or a loop over patches\n",
        "        # Advanced: Use array reshaping tricks\n",
        "        \n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Patch Embedding\n",
        "res = test_vit_patch_embedding(PatchEmbedding)\n",
        "show_result(\"Exercise 5 ‚Äì ViT Patch Embedding\", res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Complete ViT Forward Pass\n",
        "print(\"Demonstrating Vision Transformer (ViT)...\\n\")\n",
        "\n",
        "class VisionTransformer:\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, \n",
        "                 num_classes=1000, d_model=768, num_heads=12, num_layers=12, d_ff=3072):\n",
        "        \"\"\"\n",
        "        Complete Vision Transformer implementation.\n",
        "        \"\"\"\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, d_model)\n",
        "        self.n_patches = self.patch_embed.n_patches\n",
        "        \n",
        "        # CLS token (learnable parameter)\n",
        "        self.cls_token = np.random.randn(1, 1, d_model).astype(np.float32) * 0.02\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=self.n_patches + 1)\n",
        "        \n",
        "        # Transformer encoder layers\n",
        "        self.encoders = [TransformerEncoder(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
        "        \n",
        "        # Classification head\n",
        "        self.head = Linear(d_model, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through ViT.\n",
        "        \n",
        "        Args:\n",
        "            x: Input images of shape (batch_size, in_channels, img_size, img_size)\n",
        "        \n",
        "        Returns:\n",
        "            Class logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        # 1. Patch embedding\n",
        "        x = self.patch_embed.forward(x)  # (B, n_patches, d_model)\n",
        "        \n",
        "        # 2. Prepend CLS token\n",
        "        cls_tokens = np.repeat(self.cls_token, batch_size, axis=0)\n",
        "        x = np.concatenate([cls_tokens, x], axis=1)  # (B, n_patches+1, d_model)\n",
        "        \n",
        "        # 3. Add positional encoding\n",
        "        x = self.pos_encoding.forward(x)\n",
        "        \n",
        "        # 4. Pass through transformer encoders\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder.forward(x)\n",
        "        \n",
        "        # 5. Classification using CLS token\n",
        "        cls_output = x[:, 0, :]  # Take CLS token\n",
        "        logits = self.head(cls_output)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "try:\n",
        "    # Create a small ViT for demonstration\n",
        "    vit = VisionTransformer(\n",
        "        img_size=224, \n",
        "        patch_size=16, \n",
        "        num_classes=10,\n",
        "        d_model=192,  # Smaller for demo\n",
        "        num_heads=3,\n",
        "        num_layers=6,  # Fewer layers for demo\n",
        "        d_ff=768\n",
        "    )\n",
        "    \n",
        "    # Test forward pass\n",
        "    test_img = np.random.randn(2, 3, 224, 224).astype(np.float32)\n",
        "    output = vit.forward(test_img)\n",
        "    \n",
        "    print(f\"‚úì Vision Transformer working!\")\n",
        "    print(f\"\\nArchitecture:\")\n",
        "    print(f\"  - Image size: 224√ó224\")\n",
        "    print(f\"  - Patch size: 16√ó16\")\n",
        "    print(f\"  - Number of patches: {vit.n_patches}\")\n",
        "    print(f\"  - Embedding dimension: 192\")\n",
        "    print(f\"  - Attention heads: 3\")\n",
        "    print(f\"  - Transformer layers: 6\")\n",
        "    print(f\"\\nForward pass:\")\n",
        "    print(f\"  - Input: {test_img.shape}\")\n",
        "    print(f\"  - Output: {output.shape}\")\n",
        "    print(f\"\\nViT treats images as sequences of patches!\")\n",
        "    print(f\"No convolutions needed ‚Äì pure transformer architecture.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Make sure Exercise 5 is completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Comparison\n",
        "\n",
        "### Architecture Comparison\n",
        "\n",
        "| Architecture | Best For | Key Innovation | Parameters (typical) |\n",
        "|--------------|----------|----------------|---------------------|\n",
        "| **AlexNet** | Image classification | Deep CNNs, ReLU, Dropout | ~60M |\n",
        "| **ResNet** | Very deep networks | Skip connections | 25M-60M |\n",
        "| **LSTM** | Sequential data | Gating mechanism | Varies |\n",
        "| **Transformer** | Long sequences | Attention, parallelization | 100M-1B+ |\n",
        "| **ViT** | Images (with lots of data) | Patch-based transformers | 86M-632M |\n",
        "\n",
        "### When to Use What?\n",
        "\n",
        "**Use CNNs (AlexNet/ResNet) when:**\n",
        "- Working with images\n",
        "- Limited training data\n",
        "- Need translation invariance\n",
        "- Want efficiency\n",
        "\n",
        "**Use RNNs/LSTMs when:**\n",
        "- Sequential data with temporal dependencies\n",
        "- Online/streaming processing\n",
        "- Audio, time series, text (small scale)\n",
        "\n",
        "**Use Transformers when:**\n",
        "- Need long-range dependencies\n",
        "- Have lots of data and compute\n",
        "- Want parallelization\n",
        "- NLP tasks, large-scale vision\n",
        "\n",
        "### Modern Trends (2024)\n",
        "1. **Hybrid architectures**: Combining CNNs + Transformers (e.g., ConvNeXt)\n",
        "2. **Efficient transformers**: Reducing computational cost\n",
        "3. **Vision-language models**: CLIP, Flamingo (multimodal)\n",
        "4. **Foundation models**: Pre-trained on massive data, fine-tuned for tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Reflection Questions\n",
        "\n",
        "1. **Skip connections**: Why do skip connections help with training very deep networks? What problem do they solve?\n",
        "\n",
        "2. **LSTM vs Transformer**: Both handle sequential data. What are the key differences in how they process sequences? When would you choose one over the other?\n",
        "\n",
        "3. **Attention mechanism**: Explain in your own words how scaled dot-product attention works. Why is the scaling factor $\\sqrt{d_k}$ important?\n",
        "\n",
        "4. **ViT vs CNN**: Vision Transformers treat images as sequences of patches, while CNNs use convolutions. What are the trade-offs? Why does ViT need more data than CNNs?\n",
        "\n",
        "5. **Inductive biases**: CNNs have strong inductive biases (locality, translation invariance), while Transformers have fewer. What does this mean for learning and generalization?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Write your answers here._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Next Steps\n",
        "\n",
        "### For Your Projects\n",
        "1. **Start simple**: Use pre-trained models (transfer learning)\n",
        "2. **Image tasks**: Try ResNet or ViT from PyTorch/TensorFlow\n",
        "3. **Sequence tasks**: Use transformers (Hugging Face library)\n",
        "4. **Don't reinvent**: Leverage existing implementations\n",
        "\n",
        "### Further Learning\n",
        "- **Papers**: Original papers (AlexNet, ResNet, Attention is All You Need, ViT)\n",
        "- **Courses**: CS231n (Stanford), CS224n (Stanford)\n",
        "- **Implementations**: PyTorch tutorials, TensorFlow guides\n",
        "- **Practice**: Kaggle competitions, personal projects\n",
        "\n",
        "**Congratulations!** You've now seen the major architectures powering modern AI. üéâ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
