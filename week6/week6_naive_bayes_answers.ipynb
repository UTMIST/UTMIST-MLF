{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6289a598",
   "metadata": {},
   "source": [
    "# Week 6 — Naive Bayes (Generative Text Classifier)\n",
    "\n",
    "Objectives\n",
    "- Review joint probability, conditional probability, and Bayes’ theorem\n",
    "- Understand Naive Bayes as a generative classifier with conditional independence assumptions\n",
    "- Compare generative vs. discriminative models\n",
    "- Implement Bernoulli and Multinomial Naive Bayes for text classification\n",
    "- Train and evaluate on a small spam dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc2e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, sys, os\n",
    "from pprint import pprint\n",
    "\n",
    "from utils import (\n",
    "    show_result, tokenize, build_vocab, vectorize_bow, train_test_split,\n",
    "    NaiveBayesText, accuracy, confusion_matrix, tiny_spam_dataset,\n",
    "    test_exercise_1_probability, test_exercise_2_nb_fit_predict, test_exercise_3_smoothing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06559a87",
   "metadata": {},
   "source": [
    "## 1. Probability Warm‑up\n",
    "\n",
    "Definitions\n",
    "- Joint: \\(p(a,b)\\)\n",
    "- Conditional: \\(p(a\\mid b) = \\frac{p(a,b)}{p(b)}\\), with \\(p(b) > 0\\)\n",
    "- Bayes’ theorem: \\(p(a \\mid b) = \\frac{p(b \\mid a)p(a)}{p(b)}\\)\n",
    "\n",
    "Implement the functions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f0e1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] Exercise 1 – Probability\n"
     ]
    }
   ],
   "source": [
    "def joint(p_a, p_b):\n",
    "    return p_a * p_b\n",
    "\n",
    "def conditional(p_ab, p_b):\n",
    "    return p_ab / p_b\n",
    "\n",
    "def bayes(p_b_given_a, p_a, p_b):\n",
    "    return (p_b_given_a * p_a) / p_b\n",
    "\n",
    "res = test_exercise_1_probability({\"joint\": joint, \"conditional\": conditional, \"bayes\": bayes})\n",
    "show_result(\"Exercise 1 – Probability\", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950ee61",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes as a Generative Model\n",
    "\n",
    "- Model \\(p(x \\mid y)\\) and \\(p(y)\\), and compute \\(p(y \\mid x)\\) by Bayes’ rule\n",
    "- Naive assumption: features are conditionally independent given \\(y\\)\n",
    "- Bernoulli variant uses binary word presence; Multinomial uses word counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4522737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 14  |  ham=7  spam=7\n",
      "[0] hey are we still on for lunch tomorrow\n",
      "[0] please review the meeting notes from today\n",
      "[0] can you send the updated report\n"
     ]
    }
   ],
   "source": [
    "texts, labels = tiny_spam_dataset()\n",
    "print(f\"Dataset size: {len(texts)}  |  ham={sum(1 for y in labels if y==0)}  spam={sum(1 for y in labels if y==1)}\")\n",
    "for t, y in list(zip(texts, labels))[:3]:\n",
    "    print(f\"[{y}] {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d46f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(texts, min_freq=1, max_size=2000)\n",
    "X_bin = vectorize_bow(texts, vocab, binary=True)\n",
    "X_cnt = vectorize_bow(texts, vocab, binary=False)\n",
    "\n",
    "Xtr_bin, Xte_bin, ytr, yte = train_test_split(X_bin, labels, test_size=0.3, seed=7)\n",
    "Xtr_cnt, Xte_cnt, _, _ = train_test_split(X_cnt, labels, test_size=0.3, seed=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf40c94",
   "metadata": {},
   "source": [
    "## 3. Fit a Naive Bayes Classifier\n",
    "\n",
    "Complete `student_fit_func(...)`:\n",
    "1) Build vocabulary\n",
    "2) Vectorize (binary for Bernoulli, counts for Multinomial)\n",
    "3) Split into train/test\n",
    "4) Train `NaiveBayesText(mode, alpha)` and return test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1365acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] Exercise 2 – Fit & Predict\n"
     ]
    }
   ],
   "source": [
    "def student_fit_func(texts, labels, mode='bernoulli', alpha=1.0):\n",
    "    vocab = build_vocab(texts, min_freq=1, max_size=2000)\n",
    "    X = vectorize_bow(texts, vocab, binary=(mode=='bernoulli'))\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, labels, test_size=0.3, seed=7)\n",
    "    nb = NaiveBayesText(mode=mode, alpha=alpha)\n",
    "    nb.fit(Xtr, ytr)\n",
    "    ypred = nb.predict(Xte)\n",
    "    return accuracy(yte, ypred)\n",
    "\n",
    "res = test_exercise_2_nb_fit_predict(student_fit_func)\n",
    "show_result(\"Exercise 2 – Fit & Predict\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35dd11f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] Exercise 2 – Fit & Predict\n"
     ]
    }
   ],
   "source": [
    "res = test_exercise_2_nb_fit_predict(student_fit_func)\n",
    "show_result(\"Exercise 2 – Fit & Predict\", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c201b",
   "metadata": {},
   "source": [
    "## 4. Smoothing\n",
    "\n",
    "Implement `student_train_eval(alpha)` to train once (choose a mode) and return `(train_acc, test_acc)`. Then try several values of \\(\\alpha\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc48f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] Exercise 3 – Smoothing\n",
      "alpha=0.1 -> train=1.000 | test=1.000\n",
      "alpha=0.5 -> train=1.000 | test=1.000\n",
      "alpha=1.0 -> train=1.000 | test=1.000\n",
      "alpha=2.0 -> train=1.000 | test=0.500\n",
      "alpha=5.0 -> train=0.700 | test=0.250\n"
     ]
    }
   ],
   "source": [
    "def student_train_eval(alpha=1.0, mode='bernoulli'):\n",
    "    texts, labels = tiny_spam_dataset()\n",
    "    vocab = build_vocab(texts, min_freq=1, max_size=2000)\n",
    "    X = vectorize_bow(texts, vocab, binary=(mode=='bernoulli'))\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, labels, test_size=0.3, seed=7)\n",
    "    nb = NaiveBayesText(mode=mode, alpha=alpha)\n",
    "    nb.fit(Xtr, ytr)\n",
    "    tr_pred = nb.predict(Xtr)\n",
    "    te_pred = nb.predict(Xte)\n",
    "    return accuracy(ytr, tr_pred), accuracy(yte, te_pred)\n",
    "\n",
    "res = test_exercise_3_smoothing(student_train_eval)\n",
    "show_result(\"Exercise 3 – Smoothing\", res)\n",
    "\n",
    "for a in [0.1, 0.5, 1.0, 2.0, 5.0]:\n",
    "    tr, te = student_train_eval(a, mode='bernoulli')\n",
    "    print(f\"alpha={a:.1f} -> train={tr:.3f} | test={te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7312a",
   "metadata": {},
   "source": [
    "## 5. Generative vs. Discriminative (Short Answer)\n",
    "\n",
    "1) How does a generative classifier differ from a discriminative classifier?  \n",
    "2) Why can Naive Bayes be viewed as a simple text generator?  \n",
    "3) Briefly relate Naive Bayes to modern generative models (e.g., GPT).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a549826",
   "metadata": {},
   "source": [
    "**1)** Generative models learn \\(p(x\\mid y)\\) and \\(p(y)\\), then use Bayes’ rule to infer \\(p(y\\mid x)\\). Discriminative models learn \\(p(y\\mid x)\\) or a direct decision boundary.\n",
    "\n",
    "**2)** In the Multinomial variant, each class defines a word distribution; repeatedly sampling words from \\(p(w\\mid y)\\) would produce bag‑of‑words documents for that class.\n",
    "\n",
    "**3)** Both model token distributions. Naive Bayes uses strong independence assumptions over words; modern generative models (e.g., GPT) learn \\(p(x)\\) or \\(p(x\\mid\\text{prompt})\\) with deep sequence modeling that captures long‑range dependencies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
