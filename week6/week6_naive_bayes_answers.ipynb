{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6289a598",
   "metadata": {},
   "source": [
    "# Week 6 — Naive Bayes (Generative Text Classifier)\n",
    "\n",
    "Objectives\n",
    "- Review joint probability, conditional probability, and Bayes’ theorem\n",
    "- Understand Naive Bayes as a generative classifier with conditional independence assumptions\n",
    "- Compare generative vs. discriminative models\n",
    "- Implement Bernoulli and Multinomial Naive Bayes for text classification\n",
    "- Train and evaluate on a small spam dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, sys, os\n",
    "from pprint import pprint\n",
    "\n",
    "from utils import (\n",
    "    show_result, tokenize, build_vocab, vectorize_bow, train_test_split,\n",
    "    NaiveBayesText, accuracy, confusion_matrix, tiny_spam_dataset,\n",
    "    test_exercise_1_probability, test_exercise_2_nb_fit_predict, test_exercise_3_smoothing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06559a87",
   "metadata": {},
   "source": [
    "## 1. Probability Warm‑up\n",
    "\n",
    "Definitions\n",
    "- Joint: $p(a,b)$\n",
    "- Conditional: $p(a\\mid b) = \\frac{p(a,b)}{p(b)}$, with $p(b) > 0$\n",
    "- Bayes’ theorem: $p(a \\mid b) = \\frac{p(b \\mid a)p(a)}{p(b)}$\n",
    "\n",
    "Implement the functions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint(p_a, p_b):\n",
    "    return p_a * p_b\n",
    "\n",
    "def conditional(p_ab, p_b):\n",
    "    return p_ab / p_b\n",
    "\n",
    "def bayes(p_b_given_a, p_a, p_b):\n",
    "    return (p_b_given_a * p_a) / p_b\n",
    "\n",
    "res = test_exercise_1_probability({\"joint\": joint, \"conditional\": conditional, \"bayes\": bayes})\n",
    "show_result(\"Exercise 1 – Probability\", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950ee61",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes as a Generative Model\n",
    "\n",
    "- Model $p(x \\mid y)$ and $p(y)$, and compute $p(y \\mid x)$ by Bayes’ rule\n",
    "- Naive assumption: features are conditionally independent given $y$\n",
    "- Bernoulli variant uses binary word presence; Multinomial uses word counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4522737",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = tiny_spam_dataset()\n",
    "print(f\"Dataset size: {len(texts)}  |  ham={sum(1 for y in labels if y==0)}  spam={sum(1 for y in labels if y==1)}\")\n",
    "for t, y in list(zip(texts, labels))[:]:\n",
    "    print(f\"[{y}] {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d46f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(texts, min_freq=1, max_size=2000)\n",
    "print(f\"Vocabulary (size={len(vocab)}): {list(vocab)[:10]}{'...' if len(vocab) > 10 else ''}\")\n",
    "\n",
    "# Whether a word is present or not\n",
    "X_bin = vectorize_bow(texts, vocab, binary=True)\n",
    "\n",
    "# Count of words\n",
    "X_cnt = vectorize_bow(texts, vocab, binary=False)\n",
    "\n",
    "Xtr_bin, Xte_bin, ytr, yte = train_test_split(X_bin, labels, test_size=0.3, seed=7)\n",
    "Xtr_cnt, Xte_cnt, _, _ = train_test_split(X_cnt, labels, test_size=0.3, seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf40c94",
   "metadata": {},
   "source": [
    "## 3. Fit a Naive Bayes Classifier\n",
    "\n",
    "Complete `student_fit_func(...)`:\n",
    "1) Build vocabulary\n",
    "2) Vectorize (binary for Bernoulli, counts for Multinomial)\n",
    "3) Split into train/test\n",
    "4) Train `NaiveBayesText(mode, alpha)` and return test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1365acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_fit_func(texts, labels, mode='bernoulli', alpha=1.0):\n",
    "    print(\"=== Training Naive Bayes ===\")\n",
    "    vocab = build_vocab(texts, min_freq=1, max_size=2000)\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    X = vectorize_bow(texts, vocab, binary=(mode=='bernoulli'))\n",
    "    num_features = len(X[0]) if X else 0\n",
    "    print(f\"Vectorized dataset: {len(X)} samples x {num_features} features\")\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, labels, test_size=0.3, seed=7)\n",
    "    print(f\"Train size: {len(Xtr)} | Test size: {len(Xte)}\")\n",
    "    nb = NaiveBayesText(mode=mode, alpha=alpha)\n",
    "    print(f\"Training model (mode={mode}, alpha={alpha})...\")\n",
    "    nb.fit(Xtr, ytr)\n",
    "\n",
    "    print(\"Class priors (probabilities):\")\n",
    "    for c, logp in nb.class_priors.items():\n",
    "        print(f\"  y={c}: {math.exp(logp):.3f}\")\n",
    "\n",
    "    print(\"Feature likelihood samples:\")\n",
    "    vocab_items = list(vocab.items())\n",
    "    preview = min(5, len(vocab_items))\n",
    "    for idx in range(preview):\n",
    "        token, j = vocab_items[idx]\n",
    "        values = [f\"y={c}: {nb.feature_likelihoods[c][j]:.3f}\" for c in sorted(nb.feature_likelihoods)]\n",
    "        print(f\"  token='{token}' -> \" + \" | \".join(values))\n",
    "\n",
    "    ypred = nb.predict(Xte)\n",
    "    acc = accuracy(yte, ypred)\n",
    "    print(f\"Test accuracy: {acc:.3f}\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_exercise_2_nb_fit_predict(student_fit_func)\n",
    "show_result(\"Exercise 2 – Fit & Predict\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"free prize claim now\"\n",
    "nb_bin = NaiveBayesText(mode='bernoulli', alpha=1.0)\n",
    "nb_bin.fit(Xtr_bin, ytr)\n",
    "vec = vectorize_bow([sample_text], vocab, binary=True)\n",
    "proba = nb_bin.predict_proba(vec)[0]\n",
    "pred_label = nb_bin.predict(vec)[0]\n",
    "label_name = 'spam' if pred_label == 1 else 'ham'\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"Predicted label: {pred_label} ({label_name})\")\n",
    "print(f\"Posterior probabilities -> ham: {proba[0]:.3f}, spam: {proba[1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c201b",
   "metadata": {},
   "source": [
    "## 4. Smoothing\n",
    "\n",
    "Implement `student_train_eval(alpha)` to train once (choose a mode) and return `(train_acc, test_acc)`. Then try several values of $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc48f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_train_eval(alpha=1.0, mode='bernoulli'):\n",
    "    texts, labels = tiny_spam_dataset()\n",
    "    vocab = build_vocab(texts, min_freq=1, max_size=2000)\n",
    "    X = vectorize_bow(texts, vocab, binary=(mode=='bernoulli'))\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, labels, test_size=0.3, seed=7)\n",
    "    nb = NaiveBayesText(mode=mode, alpha=alpha)\n",
    "    nb.fit(Xtr, ytr)\n",
    "    tr_pred = nb.predict(Xtr)\n",
    "    te_pred = nb.predict(Xte)\n",
    "    return accuracy(ytr, tr_pred), accuracy(yte, te_pred)\n",
    "\n",
    "res = test_exercise_3_smoothing(student_train_eval)\n",
    "show_result(\"Exercise 3 – Smoothing\", res)\n",
    "\n",
    "for a in [0.1, 0.5, 1.0, 2.0, 5.0]:\n",
    "    tr, te = student_train_eval(a, mode='bernoulli')\n",
    "    print(f\"alpha={a:.1f} -> train={tr:.3f} | test={te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7312a",
   "metadata": {},
   "source": [
    "## 5. Generative vs. Discriminative (Short Answer)\n",
    "\n",
    "1) How does a generative classifier differ from a discriminative classifier?  \n",
    "2) Why can Naive Bayes be viewed as a simple text generator?  \n",
    "3) Briefly relate Naive Bayes to modern generative models (e.g., GPT).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a549826",
   "metadata": {},
   "source": [
    "**1)** Generative models learn $p(x\\mid y)$ and $p(y)$, then use Bayes’ rule to infer $p(y\\mid x)$. Discriminative models learn $p(y\\mid x)$ or a direct decision boundary.\n",
    "\n",
    "**2)** In the Multinomial variant, each class defines a word distribution; repeatedly sampling words from $p(w\\mid y)$ would produce bag‑of‑words documents for that class.\n",
    "\n",
    "**3)** Both model token distributions. Naive Bayes uses strong independence assumptions over words; modern generative models (e.g., GPT) learn $p(x)$ or $p(x\\mid\\text{prompt})$ with deep sequence modeling that captures long‑range dependencies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
