{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44febc92",
   "metadata": {},
   "source": [
    "# Week 6 — Naive Bayes (Generative Text Classifier)\n",
    "\n",
    "Objectives\n",
    "- Review joint probability, conditional probability, and Bayes’ theorem\n",
    "- Understand Naive Bayes as a generative classifier with conditional independence assumptions\n",
    "- Compare generative vs. discriminative models\n",
    "- Implement Bernoulli and Multinomial Naive Bayes for text classification\n",
    "- Train and evaluate on a small spam dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d37afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, sys, os\n",
    "from pprint import pprint\n",
    "\n",
    "from utils import (\n",
    "    show_result, tokenize, build_vocab, vectorize_bow, train_test_split,\n",
    "    NaiveBayesText, accuracy, confusion_matrix, tiny_spam_dataset,\n",
    "    test_exercise_1_probability, test_exercise_2_nb_fit_predict, test_exercise_3_smoothing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e57c0",
   "metadata": {},
   "source": [
    "## 1. Probability Warm‑up\n",
    "\n",
    "Definitions\n",
    "- Joint: \\(p(a,b)\\)\n",
    "- Conditional: \\(p(a\\mid b) = \\frac{p(a,b)}{p(b)}\\), with \\(p(b) > 0\\)\n",
    "- Bayes’ theorem: \\(p(a \\mid b) = \\frac{p(b \\mid a)p(a)}{p(b)}\\)\n",
    "\n",
    "Implement the functions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03653be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the following functions.\n",
    "def joint(p_a, p_b):\n",
    "    \"\"\"Assume independence: p(a,b) = p(a)*p(b).\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def conditional(p_ab, p_b):\n",
    "    \"\"\"p(a|b) = p(a,b) / p(b), assuming p(b) > 0.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def bayes(p_b_given_a, p_a, p_b):\n",
    "    \"\"\"Bayes' theorem: p(a|b) = p(b|a) p(a) / p(b).\"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8019d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAIL] Exercise 1 – Probability | joint runtime error: \n"
     ]
    }
   ],
   "source": [
    "res = test_exercise_1_probability({\"joint\": joint, \"conditional\": conditional, \"bayes\": bayes})\n",
    "show_result(\"Exercise 1 – Probability\", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809c7b6",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes as a Generative Model\n",
    "\n",
    "- Model \\(p(x \\mid y)\\) and \\(p(y)\\), and compute \\(p(y \\mid x)\\) by Bayes’ rule\n",
    "- Naive assumption: features are conditionally independent given \\(y\\)\n",
    "- Bernoulli variant uses binary word presence; Multinomial uses word counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c50baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 14  |  ham=7  spam=7\n",
      "[0] hey are we still on for lunch tomorrow\n",
      "[0] please review the meeting notes from today\n",
      "[0] can you send the updated report\n"
     ]
    }
   ],
   "source": [
    "texts, labels = tiny_spam_dataset()\n",
    "print(f\"Dataset size: {len(texts)}  |  ham={sum(1 for y in labels if y==0)}  spam={sum(1 for y in labels if y==1)}\")\n",
    "for t, y in list(zip(texts, labels))[:3]:\n",
    "    print(f\"[{y}] {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7263a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(texts, min_freq=1, max_size=2000)\n",
    "X_bin = vectorize_bow(texts, vocab, binary=True)\n",
    "X_cnt = vectorize_bow(texts, vocab, binary=False)\n",
    "\n",
    "Xtr_bin, Xte_bin, ytr, yte = train_test_split(X_bin, labels, test_size=0.3, seed=7)\n",
    "Xtr_cnt, Xte_cnt, _, _ = train_test_split(X_cnt, labels, test_size=0.3, seed=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2035d94",
   "metadata": {},
   "source": [
    "## 3. Fit a Naive Bayes Classifier\n",
    "\n",
    "Complete `student_fit_func(...)`:\n",
    "1) Build vocabulary\n",
    "2) Vectorize (binary for Bernoulli, counts for Multinomial)\n",
    "3) Split into train/test\n",
    "4) Train `NaiveBayesText(mode, alpha)` and return test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ac7d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_fit_func(texts, labels, mode='bernoulli', alpha=1.0):\n",
    "    \"\"\"\n",
    "    Returns test accuracy on the tiny dataset.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facd47db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAIL] Exercise 2 – Fit & Predict | runtime error: \n"
     ]
    }
   ],
   "source": [
    "res = test_exercise_2_nb_fit_predict(student_fit_func)\n",
    "show_result(\"Exercise 2 – Fit & Predict\", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f4400",
   "metadata": {},
   "source": [
    "## 4. Smoothing\n",
    "\n",
    "Implement `student_train_eval(alpha)` to train once (choose a mode) and return `(train_acc, test_acc)`. Then try several values of \\(\\alpha\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ed84952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAIL] Exercise 3 – Smoothing | runtime error: \n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m show_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExercise 3 – Smoothing\u001b[39m\u001b[38;5;124m\"\u001b[39m, res)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m5.0\u001b[39m]:\n\u001b[1;32m---> 11\u001b[0m     tr, te \u001b[38;5;241m=\u001b[39m \u001b[43mstudent_train_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbernoulli\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> train=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | test=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mte\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m, in \u001b[0;36mstudent_train_eval\u001b[1;34m(alpha, mode)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstudent_train_eval\u001b[39m(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbernoulli\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Train Naive Bayes with the given alpha; return (train_acc, test_acc).\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def student_train_eval(alpha=1.0, mode='bernoulli'):\n",
    "    \"\"\"\n",
    "    Train Naive Bayes with the given alpha; return (train_acc, test_acc).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "res = test_exercise_3_smoothing(student_train_eval)\n",
    "show_result(\"Exercise 3 – Smoothing\", res)\n",
    "\n",
    "for a in [0.1, 0.5, 1.0, 2.0, 5.0]:\n",
    "    tr, te = student_train_eval(a, mode='bernoulli')\n",
    "    print(f\"alpha={a:.1f} -> train={tr:.3f} | test={te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd6887",
   "metadata": {},
   "source": [
    "## 5. Generative vs. Discriminative (Short Answer)\n",
    "\n",
    "1) How does a generative classifier differ from a discriminative classifier?  \n",
    "2) Why can Naive Bayes be viewed as a simple text generator?  \n",
    "3) Briefly relate Naive Bayes to modern generative models (e.g., GPT).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cba0ef",
   "metadata": {},
   "source": [
    "_Answer here._\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
