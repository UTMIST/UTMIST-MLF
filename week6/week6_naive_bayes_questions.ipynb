{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44febc92",
   "metadata": {},
   "source": [
    "# Week 6 — Naive Bayes (Generative Text Classifier)\n",
    "\n",
    "Objectives\n",
    "- Review joint probability, conditional probability, and Bayes’ theorem\n",
    "- Understand Naive Bayes as a generative classifier with conditional independence assumptions\n",
    "- Compare generative vs. discriminative models\n",
    "- Implement Bernoulli and Multinomial Naive Bayes for text classification\n",
    "- Train and evaluate on a small spam dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d37afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, sys, os\n",
    "from pprint import pprint\n",
    "\n",
    "from utils import (\n",
    "    show_result, tokenize, build_vocab, vectorize_bow, train_test_split,\n",
    "    NaiveBayesText, accuracy, confusion_matrix, tiny_spam_dataset,\n",
    "    test_exercise_1_probability, test_exercise_2_nb_fit_predict, test_exercise_3_smoothing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e57c0",
   "metadata": {},
   "source": [
    "## 1. Probability Warm‑up\n",
    "\n",
    "Definitions\n",
    "- Joint: $p(a,b)$\n",
    "- Conditional: $p(a\\mid b) = \\frac{p(a,b)}{p(b)}$, with $p(b) > 0$\n",
    "- Bayes’ theorem: $p(a \\mid b) = \\frac{p(b \\mid a)p(a)}{p(b)}$\n",
    "\n",
    "Implement the functions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03653be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the following functions.\n",
    "def joint(p_a, p_b):\n",
    "    \"\"\"Assume independence: p(a,b) = p(a)*p(b).\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def conditional(p_ab, p_b):\n",
    "    \"\"\"p(a|b) = p(a,b) / p(b), assuming p(b) > 0.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def bayes(p_b_given_a, p_a, p_b):\n",
    "    \"\"\"Bayes' theorem: p(a|b) = p(b|a) p(a) / p(b).\"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8019d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_exercise_1_probability({\"joint\": joint, \"conditional\": conditional, \"bayes\": bayes})\n",
    "show_result(\"Exercise 1 – Probability\", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809c7b6",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes as a Generative Model\n",
    "\n",
    "- Model $p(x \\mid y)$ and $p(y)$, and compute $p(y \\mid x)$ by Bayes’ rule\n",
    "- Naive assumption: features are conditionally independent given $y$\n",
    "- Bernoulli variant uses binary word presence; Multinomial uses word counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c50baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = tiny_spam_dataset()\n",
    "print(f\"Dataset size: {len(texts)}  |  ham={sum(1 for y in labels if y==0)}  spam={sum(1 for y in labels if y==1)}\")\n",
    "for t, y in list(zip(texts, labels)):\n",
    "    print(f\"[{y}] {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7263a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(texts, min_freq=1, max_size=2000)\n",
    "print(f\"Vocabulary (size={len(vocab)}): {list(vocab)[:10]}{'...' if len(vocab) > 10 else ''}\")\n",
    "\n",
    "# Whether a word is present or not\n",
    "X_bin = vectorize_bow(texts, vocab, binary=True)\n",
    "\n",
    "# Count of words\n",
    "X_cnt = vectorize_bow(texts, vocab, binary=False)\n",
    "\n",
    "Xtr_bin, Xte_bin, ytr, yte = train_test_split(X_bin, labels, test_size=0.3, seed=7)\n",
    "Xtr_cnt, Xte_cnt, _, _ = train_test_split(X_cnt, labels, test_size=0.3, seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2035d94",
   "metadata": {},
   "source": [
    "## 3. Fit a Naive Bayes Classifier\n",
    "\n",
    "Complete `student_fit_func(...)`:\n",
    "1) Build vocabulary\n",
    "2) Vectorize (binary for Bernoulli, counts for Multinomial)\n",
    "3) Split into train/test\n",
    "4) Train `NaiveBayesText(mode, alpha)` and return test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_fit_func(texts, labels, mode='bernoulli', alpha=1.0):\n",
    "    \"\"\"\n",
    "    Returns test accuracy on the tiny dataset.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_exercise_2_nb_fit_predict(student_fit_func)\n",
    "show_result(\"Exercise 2 – Fit & Predict\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"free prize claim now\"\n",
    "nb_bin = NaiveBayesText(mode='bernoulli', alpha=1.0)\n",
    "nb_bin.fit(Xtr_bin, ytr)\n",
    "vec = vectorize_bow([sample_text], vocab, binary=True)\n",
    "proba = nb_bin.predict_proba(vec)[0]\n",
    "pred_label = nb_bin.predict(vec)[0]\n",
    "label_name = 'spam' if pred_label == 1 else 'ham'\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"Predicted label: {pred_label} ({label_name})\")\n",
    "print(f\"Posterior probabilities -> ham: {proba[0]:.3f}, spam: {proba[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f4400",
   "metadata": {},
   "source": [
    "## 4. Smoothing\n",
    "\n",
    "Implement `student_train_eval(alpha)` to train once (choose a mode) and return `(train_acc, test_acc)`. Then try several values of $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed84952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_train_eval(alpha=1.0, mode='bernoulli'):\n",
    "    \"\"\"\n",
    "    Train Naive Bayes with the given alpha; return (train_acc, test_acc).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "res = test_exercise_3_smoothing(student_train_eval)\n",
    "show_result(\"Exercise 3 – Smoothing\", res)\n",
    "\n",
    "for a in [0.1, 0.5, 1.0, 2.0, 5.0]:\n",
    "    tr, te = student_train_eval(a, mode='bernoulli')\n",
    "    print(f\"alpha={a:.1f} -> train={tr:.3f} | test={te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd6887",
   "metadata": {},
   "source": [
    "## 5. Generative vs. Discriminative (Short Answer)\n",
    "\n",
    "1) How does a generative classifier differ from a discriminative classifier?  \n",
    "2) Why can Naive Bayes be viewed as a simple text generator?  \n",
    "3) Briefly relate Naive Bayes to modern generative models (e.g., GPT).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cba0ef",
   "metadata": {},
   "source": [
    "_Answer here._\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
