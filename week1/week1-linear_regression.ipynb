{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8993f69",
   "metadata": {},
   "source": [
    "# PyTorch Fundamentals \n",
    "\n",
    "PyTorch is a deep learning framework that uses tensors as its core data structure. A tensor is essentially a multi-dimensional array (just like a NumPy array) that can run on GPU for accelerated computing. Unlike NumPy, PyTorch tensors also support automatic differentiation, which is crucial for machine learning. In practice, you can think of tensors as flexible containers for data ‚Äì vectors, matrices, images, etc. ‚Äì that PyTorch can track through computations to compute gradients for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595a191a",
   "metadata": {},
   "source": [
    "## 1.1 Tensors and Tensor Operations\n",
    "Let‚Äôs begin by importing PyTorch and creating some tensors. We‚Äôll see how to perform basic operations with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Creating tensors (PyTorch uses torch.Tensor class)\n",
    "# 1. Scalar (0-d tensor)\n",
    "scalar = torch.tensor(3.14)\n",
    "print(\"Scalar tensor:\", scalar)\n",
    "\n",
    "# 2. 1-D Vector\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Vector tensor:\", vector)\n",
    "\n",
    "# 3. 2-D Matrix \n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "print(\"Matrix tensor:\\n\", matrix)\n",
    "\n",
    "# You can check tensor attributes like shape and type:\n",
    "print(\"Vector shape:\", vector.shape)\n",
    "print(\"Matrix dtype:\", matrix.dtype)\n",
    "\n",
    "# Basic tensor operations (similar to NumPy):\n",
    "x = torch.tensor([10.0, 20.0, 30.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(\"x + y =\", x + y)           # element-wise addition\n",
    "print(\"x * y =\", x * y)           # element-wise multiplication\n",
    "print(\"x dot y =\", torch.dot(x, y))  # dot product\n",
    "print(\"x mean =\", x.mean())       # compute mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dede4f",
   "metadata": {},
   "source": [
    "In the code above, we created:\n",
    "\n",
    "* A **scalar** tensor (just a single number).\n",
    "* A **vector** (1-dimensional tensor) with 3 elements.\n",
    "* A **matrix** (2-dimensional tensor) with 2 rows and 3 columns.\n",
    "\n",
    "We then performed some basic operations: element-wise addition, multiplication, dot product, etc. PyTorch overloads standard Python arithmetic operators for tensors, so `x + y` works as you‚Äôd expect (adding corresponding elements). You can also call functions like `torch.dot` for dot product or tensor methods like `x.mean()`.\n",
    "\n",
    "Notice that PyTorch tensors have attributes like shape (also called size) and dtype (data type). By default, torch uses 32-bit floating point (`torch.float32`) for many operations, which is usually what we want for neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f215ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1.2 Autograd and Gradients\n",
    "One of the most powerful features of PyTorch is its **autograd** (automatic differentiation) engine. Autograd allows PyTorch to **compute gradients automatically** for any tensor operations, which is essential for model training. When we set `requires_grad=True` on a tensor, PyTorch will record all operations involving that tensor and build a computational graph. Then, by calling `.backward()`, it computes the gradient of some final result (like a loss) with respect to that tensor (and any other tensors with `requires_grad`).\n",
    "\n",
    "In machine learning, we use gradients to update model parameters in the direction that reduces the loss (via gradient descent). Autograd frees us from manually computing these derivatives.\n",
    "\n",
    "Let's see a simple example of **autograd** in action.  \n",
    "We will create a tensor `x`, enable gradients on it, and then do a simple operation:\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "We know the derivative:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 2x\n",
    "$$\n",
    "\n",
    "So at:\n",
    "\n",
    "$$\n",
    "x = 2\n",
    "$$\n",
    "\n",
    "we expect the gradient to be:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 4\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using autograd to compute a derivative\n",
    "x = torch.tensor(2.0, requires_grad=True)   # define a tensor with gradients tracking\n",
    "y = x ** 2                                  # y = x^2\n",
    "y.backward()                                # compute dy/dx\n",
    "print(\"x.grad =\", x.grad)                   # should be 4, since dy/dx = 2*x at x=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e0e64",
   "metadata": {},
   "source": [
    "When we run this, PyTorch will compute the gradient:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "and store it in `x.grad`. You should see: \n",
    "$$x.grad = tensor(4.)$$ \n",
    "confirming that the gradient is 4 as expected.  \n",
    "\n",
    "Under the hood, PyTorch built a graph of the operation:\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "and used the chain rule to get the derivative. This automatic gradient computation is what powers neural network training in PyTorch ([docs](https://pytorch.org/docs/)).\n",
    "\n",
    "---\n",
    "\n",
    "**Key concept:** In training, we forward propagate inputs through the model to get an output and a loss, and then backpropagate to get gradients of the loss w.r.t. each parameter ([docs](https://pytorch.org/docs/)).  \n",
    "PyTorch‚Äôs autograd does the backpropagation for us ‚Äì we just call `.backward()` on the loss tensor, and it computes all the necessary gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42773bb",
   "metadata": {},
   "source": [
    "Now let's see how **autograd** works on a `(2, )` tensors\n",
    "\n",
    "\n",
    "Let's see a simple example of **autograd** in action.  \n",
    "We will create a tensor $[x_1, x_2]$, enable gradients on it, and then do a simple operation:\n",
    "\n",
    "$$\n",
    "y = {x_1}^2 + {x_2}^2\n",
    "$$\n",
    "\n",
    "We know the derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_1} = 2x_1\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_2} = 2x_2\n",
    "$$\n",
    "\n",
    "So at:\n",
    "\n",
    "$$\n",
    "x_1 = 2\n",
    "$$\n",
    "$$\n",
    "x_2 = 3\n",
    "$$\n",
    "\n",
    "we expect the gradient to be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_1} = 4\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_2} = 6\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: What about a tensor of (2,)\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)   # define a tensor with gradients tracking\n",
    "y = (x ** 2).sum()                          # y = x1^2 + x2^2\n",
    "y.backward()                                # compute dy/dx\n",
    "print(\"x.grad =\", x.grad)                   # should be 4, since dy/dx = 2*x at x=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756eb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what's the gradient of this?\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)   # define a tensor with gradients tracking\n",
    "y = (x ** 2).mean()                         # Note how the function is changed!!!\n",
    "y.backward()                                # compute dy/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc213d5",
   "metadata": {},
   "source": [
    "# 1.3 Basic Training Workflow in PyTorch\n",
    "\n",
    "Now that we know about tensors and autograd, let‚Äôs outline a basic workflow for training a machine learning model (like a neural network) in PyTorch. The typical steps are:\n",
    "\n",
    "1. **Prepare the data** ‚Äì load your dataset and convert it into tensors (and usually create a `DataLoader` for batching, though for simplicity we‚Äôll skip `DataLoader` for now).\n",
    "2. **Define the model** ‚Äì specify the architecture. In PyTorch, you can either use built-in layers from `torch.nn` or define your own `nn.Module`.  \n",
    "   Example: a simple linear model can be defined with `nn.Linear`.\n",
    "3. **Define the loss function** ‚Äì e.g. Mean Squared Error for regression, or Cross-Entropy for classification. PyTorch provides many losses in `torch.nn` (like `nn.MSELoss`).\n",
    "4. **Define the optimizer** ‚Äì e.g. Stochastic Gradient Descent (SGD), Adam, etc., from `torch.optim`. You tell it which model parameters to update and the learning rate.\n",
    "5. **Training loop** ‚Äì iterate over the data for multiple epochs:  \n",
    "   - Forward pass: `model(x)` to get predictions  \n",
    "   - Compute loss  \n",
    "   - `loss.backward()` to get gradients  \n",
    "   - `optimizer.step()` to update parameters  \n",
    "   - Don‚Äôt forget `optimizer.zero_grad()` at each iteration to reset gradients  \n",
    "\n",
    "We will see these steps in action when we implement **linear regression** below.  \n",
    "For a small problem you might not use batches, but the loop structure is the same.  \n",
    "PyTorch will handle the gradient calculations and weight updates, so you can focus on the model logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f2556",
   "metadata": {},
   "source": [
    "# 2. Linear Regression: Your First Model\n",
    "\n",
    "Linear regression is the simplest machine learning model for supervised learning.  \n",
    "It assumes a linear relationship between the input variables and the output (target).  \n",
    "\n",
    "In the case of a single input variable (univariate linear regression), this means we try to fit a straight line:\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "that best predicts the target $y$ from the input $x$.  \n",
    "The parameters $w$ (weight or slope) and $b$ (bias or intercept) define the line.  \n",
    "\n",
    "Linear regression finds the values of $w$ and $b$ that minimize the prediction errors on the training data ([source](https://oi.readthedocs.io)).\n",
    "\n",
    "> **Statistics view:**  \n",
    "> ‚ÄúLinear regression is a linear approach to modeling the relationship between a scalar response (dependent variable) and one or more explanatory variables (independent variables)‚Äù ([source](https://oi.readthedocs.io)).\n",
    "\n",
    "Here we'll focus on **simple linear regression** (one independent variable) for clarity.  \n",
    "Despite its simplicity, linear regression introduces the **core ideas of model training**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d712e",
   "metadata": {},
   "source": [
    "## 2.1 Problem Setup ‚Äì A Simple Dataset\n",
    "\n",
    "To make this practical, let's consider an example scenario:  \n",
    "We want to predict a student's exam score based on how many hours they studied.  \n",
    "\n",
    "- Input ($x$): hours studied  \n",
    "- Target ($y$): exam score  \n",
    "\n",
    "We expect a **positive linear relationship**: more hours ‚Üí higher score (up to a point).  \n",
    "Of course, real data will have some noise.\n",
    "\n",
    "Recall from the lecture slides: the (perhaps tongue-in-cheek) idea that your chances of landing an AI/ML internship might increase linearly with the number of MLF sessions you attend. That is also a one-dimensional linear regression problem. Here, we'll use the study hours vs score scenario, but the mechanics are identical.\n",
    "\n",
    "Let's **generate a synthetic dataset** for study hours vs exam score. We will assume the true relationship is roughly `score = 5 * hours + 50` (just an arbitrary linear trend), and add some random noise to simulate variability among students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838426d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate 100 random study hours between 0 and 10\n",
    "N = 100\n",
    "hours = 10 * torch.rand(N)  # uniform random in [0, 10]\n",
    "# Generate scores with a linear relation to hours plus some noise\n",
    "true_w = 5.0    # true slope (points per hour)\n",
    "true_b = 50.0   # true intercept (base score with 0 hours)\n",
    "noise = torch.randn(N) * 5.0  # random noise (standard deviation 5)\n",
    "scores = true_w * hours + true_b + noise\n",
    "\n",
    "# Quick visualization of the data\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(hours.numpy(), scores.numpy(), color='orange')\n",
    "plt.xlabel(\"Hours Studied\")\n",
    "plt.ylabel(\"Exam Score\")\n",
    "plt.title(\"Study Hours vs Exam Score (synthetic data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbc6fa",
   "metadata": {},
   "source": [
    "In this code, we created `hours` (our x values) and `scores` (our y values).  \n",
    "We defined the \"true\" underlying line as:\n",
    "\n",
    "$$\n",
    "y = 5 \\cdot x + 50\n",
    "$$\n",
    "\n",
    "and added Gaussian noise (`torch.randn`) to make the data imperfect.  \n",
    "The plot should show a scattered cloud of points roughly forming an upward sloping line. (Each point represents one student.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1eed25",
   "metadata": {},
   "source": [
    "## 2.2 The Linear Regression Model and Loss Function\n",
    "\n",
    "Our goal is to **learn** the parameters of a line:\n",
    "\n",
    "$$\n",
    "y = wx + b\n",
    "$$\n",
    "\n",
    "that best fits this data. ‚ÄúBest fit‚Äù in a least-squares sense means the line should minimize the **Mean Squared Error (MSE)** between its predictions and the actual data points.  \n",
    "The MSE loss for a dataset is defined as:\n",
    "\n",
    "$$\n",
    "MSE(w, b) = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}^{(i)} - y^{(i)})^2,\n",
    "$$\n",
    "\n",
    "where $\\hat{y}^{(i)} = w x^{(i)} + b$ is the prediction for the i-th data point, and $y^{(i)}$ is the true target.  \n",
    "MSE essentially measures the average squared error ‚Äì we square the differences so that negative/positive errors don‚Äôt cancel out, and to penalize large errors more.\n",
    "\n",
    "---\n",
    "\n",
    "**Our task:** find $w$ and $b$ that minimize MSE.  \n",
    "For linear regression, there is a known closed-form solution, but here we'll solve it using **gradient descent**, which is the basis for training most ML models.  \n",
    "\n",
    "Gradient descent is an iterative optimization algorithm that takes steps proportional to the negative of the gradient of the loss function ([oi.readthedocs.io](https://oi.readthedocs.io)).  \n",
    "In simpler terms, we repeatedly adjust $w$ and $b$ in the direction that lowers the MSE.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient descent update rules:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial MSE}{\\partial w}, \\quad \n",
    "b := b - \\alpha \\frac{\\partial MSE}{\\partial b},\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the **learning rate** ‚Äì a small positive number that controls how big a step we take on each iteration.  \n",
    "If $\\alpha$ is too large, we might overshoot the minimum and diverge; if it‚Äôs too small, learning will be very slow.  \n",
    "Choosing a good learning rate often requires tuning.\n",
    "\n",
    "---\n",
    "\n",
    "For our linear model, we can derive the gradients of MSE w.r.t. $w$ and $b$.  \n",
    "If \n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum (wx_i + b - y_i)^2,\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{2}{N} \\sum_{i=1}^N (wx_i + b - y_i)x_i, \\quad\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{2}{N} \\sum_{i=1}^N (wx_i + b - y_i).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "You don‚Äôt need to memorize these formulas ‚Äì we‚Äôll implement them in code.  \n",
    "But it‚Äôs good to see that computing these involves summing over all data points.\n",
    "\n",
    "---\n",
    "\n",
    "Now, let's implement **manual gradient descent** to find $w$ and $b$ that fit our data.  \n",
    "We‚Äôll start with random initial guesses for $w$ and $b$, then iteratively update them using the gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a7daa",
   "metadata": {},
   "source": [
    "## 2.3 Manual Gradient Descent Implementation (Exercise)\n",
    "\n",
    "We will go through the training loop step by step. To help you learn, parts of the code are left as **TODO** for you to fill in.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Initialize parameters $w$ and $b$ randomly.  \n",
    "2. For a number of epochs (iterations):  \n",
    "   - Compute predictions $\\hat{y} = wx + b$ for all data points.  \n",
    "   - Compute the MSE loss (average of squared errors).  \n",
    "   - Compute the gradients $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ using the formulas above.  \n",
    "   - Update $w$ and $b$ with the gradient descent step.  \n",
    "3. Print the learned parameters and final loss.  \n",
    "\n",
    "---\n",
    "\n",
    "Let's code this. Remember, the goal is to reduce the loss each step by nudging $w$ and $b$ in the opposite direction of the gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b06a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import grading utilities\n",
    "from utils import grade_both_gradients, print_grading_result\n",
    "\n",
    "# Initialize parameters\n",
    "w = torch.randn(1, requires_grad=False)  # start with a random weight\n",
    "b = torch.randn(1, requires_grad=False)  # start with a random bias\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "loss_values = []\n",
    "\n",
    "# For grading - we'll compute gradients once at the beginning\n",
    "# Forward pass: compute predictions and loss\n",
    "y_pred = w * hours + b                     # model predictions for all N data points\n",
    "error = y_pred - scores                    # error term (prediction - true value)\n",
    "loss = torch.mean(error ** 2)              # MSE loss for this epoch\n",
    "\n",
    "print(\"üéØ EXERCISE: Derive and implement the gradient calculations!\")\n",
    "print(\"Follow the mathematical derivation steps in the markdown cell below.\")\n",
    "print(\"Then implement your derived formulas in PyTorch code.\")\n",
    "print()\n",
    "\n",
    "# TODO: Fill in your gradient calculations here\n",
    "grad_w = None  # TODO: compute gradient w.rt. w (use (error * hours))\n",
    "grad_b = None  # TODO: compute gradient w.rt. b (use error)\n",
    "\n",
    "# Grade the student's answers\n",
    "if grad_w is not None or grad_b is not None:\n",
    "    result = grade_both_gradients(error, hours, grad_w, grad_b)\n",
    "    print_grading_result(result)\n",
    "    \n",
    "    if result['correct']:\n",
    "        print(\"\\nüéâ Great job! Now let's run the full training loop...\")\n",
    "        \n",
    "        # Reset parameters for actual training\n",
    "        w = torch.randn(1, requires_grad=False)\n",
    "        b = torch.randn(1, requires_grad=False)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass: compute predictions and loss\n",
    "            y_pred = w * hours + b\n",
    "            error = y_pred - scores\n",
    "            loss = torch.mean(error ** 2)\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Compute gradients using the formulas you derived above\n",
    "            grad_w = 2.0 * torch.sum(error * hours) / hours.numel()\n",
    "            grad_b = 2.0 * torch.sum(error) / hours.numel()\n",
    "            \n",
    "            # Update parameters using gradient descent\n",
    "            w = w - learning_rate * grad_w\n",
    "            b = b - learning_rate * grad_b\n",
    "\n",
    "        # After training, let's see the results\n",
    "        print(f\"\\nLearned weight w: {w.item():.3f}\")\n",
    "        print(f\"Learned bias b: {b.item():.3f}\")\n",
    "        print(f\"Final training MSE: {loss_values[-1]:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Please fix your gradient calculations and try again!\")\n",
    "        print(\"Hint: Make sure to use the correct formulas and tensor operations.\")\n",
    "else:\n",
    "    print(\"‚ùå Please compute both grad_w and grad_b before running the training loop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b9647",
   "metadata": {},
   "source": [
    "## üéØ Interactive Exercise: Manual Gradient Descent\n",
    "\n",
    "**Your Task:** Fill in the TODO lines in the cell above to calculate `grad_w` and `grad_b`. \n",
    "\n",
    "### üìö **Deriving the Gradients (Your Turn!)**\n",
    "\n",
    "Recall that we want to minimize the MSE loss:\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} (wx_i + b - y_i)^2$$\n",
    "\n",
    "**Step 1:** Take the partial derivative of L with respect to w:\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} (wx_i + b - y_i)^2 \\right]$$\n",
    "\n",
    "**Step 2:** Apply the chain rule and simplify...\n",
    "\n",
    "**Step 3:** Do the same for the partial derivative with respect to b:\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} (wx_i + b - y_i)^2 \\right]$$\n",
    "\n",
    "**Hints:**\n",
    "- Use the chain rule: $\\frac{d}{dx}[f(x)]^2 = 2f(x) \\cdot \\frac{df}{dx}$\n",
    "- Remember that $\\frac{\\partial}{\\partial w}(wx_i + b) = x_i$ and $\\frac{\\partial}{\\partial b}(wx_i + b) = 1$\n",
    "- The error term $(wx_i + b - y_i)$ is already computed as `error = y_pred - scores`\n",
    "- Use PyTorch operations: `torch.sum()`, `hours.numel()` for N\n",
    "\n",
    "**How it works:**\n",
    "1. Derive the gradient formulas mathematically (steps above)\n",
    "2. Implement them in PyTorch code in the cell above\n",
    "3. Run the cell - it will automatically grade your answers\n",
    "4. If correct ‚úÖ, the training loop will run automatically\n",
    "5. If incorrect ‚ùå, check your derivation and try again\n",
    "\n",
    "**Testing:** Use the helper cell below to test your answers before submitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073863c",
   "metadata": {},
   "source": [
    "If your implementation is correct, the loss should steadily decrease.  \n",
    "The learned `w` and `b` should end up near the true values ~5 and ~50 (they won‚Äôt be exact due to noise).  \n",
    "The final MSE should also be much lower than the initial MSE.  \n",
    "(Remember, initially $w, b$ were random, so the starting error was large)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036b19b",
   "metadata": {},
   "source": [
    "**Understanding the output:**  \n",
    "You might see something like: `Learned weight w: 4.8`, `Learned bias b: 51.2` which is close to the true slope 5.0 and intercept 50.0 we used to generate the data.  \n",
    "The final MSE might be around 20‚Äì30 ${points}^2$ (since our noise had std 5, the best possible MSE is around 25).  \n",
    "This means on average the prediction error is about 5 points, which makes sense given the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3c318",
   "metadata": {},
   "source": [
    "Why didn‚Äôt we get **exactly** 5.0 and 50.0?  \n",
    "Because the data had random noise ‚Äì there isn‚Äôt a single perfect line that goes through all points.  \n",
    "Gradient descent finds the best compromise line that minimizes squared errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce46ea",
   "metadata": {},
   "source": [
    "It‚Äôs worth noting that for a simple linear regression, we **could** solve for the optimal $w, b$ analytically (using a formula or linear algebra).  \n",
    "However, in more complex models or higher dimensions, that isn‚Äôt feasible.  \n",
    "Instead, we rely on iterative methods like gradient descent for thousands or millions of parameters.  \n",
    "As the lecture pointed out, *‚Äúno one does this math by hand‚Ä¶ you can do this through coding!‚Äù*.  \n",
    "Now you‚Äôve seen how to do exactly that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf229e5",
   "metadata": {},
   "source": [
    "## 2.4 Visualizing the Results\n",
    "\n",
    "Let's visualize what our learned model looks like compared to the data, and also see how the loss decreased over training. We‚Äôll plot the data points and the learned regression line, as well as a curve of the MSE loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ad0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the learned line, and the loss curve\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Left: scatter plot of data and fitted line\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(hours.numpy(), scores.numpy(), color='blue', label='Data')\n",
    "# plot the learned line:\n",
    "x_line = torch.tensor([0.0, 10.0])  # two end-points (min and max hours)\n",
    "y_line = w * x_line + b\n",
    "plt.plot(x_line.numpy(), y_line.detach().numpy(), color='red', label=f'Fit: y={w.item():.1f}x+{b.item():.1f}')\n",
    "plt.xlabel(\"Hours Studied\")\n",
    "plt.ylabel(\"Exam Score\")\n",
    "plt.title(\"Data and Fitted Line\")\n",
    "plt.legend()\n",
    "\n",
    "# Right: loss curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(epochs), loss_values, color='purple')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c27362",
   "metadata": {},
   "source": [
    "*Figure: (Left) Scatter plot of the study hours vs exam scores dataset (blue points) and the linear regression line fitted by our model (red line). (Right) The MSE loss decreasing over training iterations. The loss curve shows that gradient descent successfully reduced the error over time, leveling off as it nears the minimum.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68383f",
   "metadata": {},
   "source": [
    "You can see that the red line fits the data trend quite well.  \n",
    "Most points are fairly close to the line, which indicates our model captured the relationship.  \n",
    "The loss curve on the right starts higher and slopes downward, confirming that our training process was working ‚Äì the errors got smaller and smaller.  \n",
    "The curve typically flattens out when further improvements are minimal (convergence).\n",
    "\n",
    "---\n",
    "\n",
    "Try experimenting with the learning rate or number of epochs and re-running the training to see how it affects the loss curve.  \n",
    "\n",
    "For example, what if you use: `learning_rate = 0.1` (ten times larger)? You might observe the loss diverging or oscillating if the rate is too large. This highlights why picking a proper learning rate is important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a511b",
   "metadata": {},
   "source": [
    "# 2.5 Using PyTorch `nn` and `optim` (Built-in Training)\n",
    "\n",
    "We just implemented linear regression ‚Äúfrom scratch‚Äù using basic PyTorch tensor operations. While this is great for learning, in practice we can leverage PyTorch‚Äôs high-level APIs to do the same job more succinctly. PyTorch provides:\n",
    "\n",
    "- `nn.Linear` ‚Äì a linear layer (module) that internally holds a weight and bias parameter and computes $wx + b$.\n",
    "- **Loss functions** in `torch.nn`, like `nn.MSELoss` which we can use for MSE.\n",
    "- **Optimizers** in `torch.optim` that update parameters for us, e.g. `torch.optim.SGD`.\n",
    "\n",
    "We will now redo the linear regression using these tools. This will look more like typical PyTorch code in research or production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple linear regression model using nn.Linear\n",
    "model = nn.Linear(1, 1)  # 1 input feature, 1 output feature\n",
    "\n",
    "# Define the Mean Squared Error loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (Stochastic Gradient Descent) to update our model parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare data shape: currently hours and scores are shape [N], we need [N,1] for model\n",
    "X_train = hours.view(-1, 1)   # reshape to (100, 1)\n",
    "y_train = scores.view(-1, 1)\n",
    "\n",
    "# Training loop (similar structure, but using PyTorch machinery)\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: compute predictions\n",
    "    y_pred = model(X_train)               # model does: w*x + b internally\n",
    "    loss = loss_fn(y_pred, y_train)      # compute MSE loss between predictions and true scores\n",
    "    \n",
    "    # Backward pass and optimization step\n",
    "    optimizer.zero_grad()   # reset gradients from previous step\n",
    "    loss.backward()         # compute gradients of loss w.rt. model parameters\n",
    "    optimizer.step()        # update parameters (w and b) using those gradients\n",
    "\n",
    "# After training, examine the learned parameters\n",
    "w_hat = model.weight.item()\n",
    "b_hat = model.bias.item()\n",
    "print(f\"Model learned w = {w_hat:.3f}, b = {b_hat:.3f}\")\n",
    "print(f\"Final loss = {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3806c1",
   "metadata": {},
   "source": [
    "In a few lines, we have done the same thing as before! Let‚Äôs break down what happened:\n",
    "\n",
    "- `nn.Linear(1,1)` creates a linear layer (essentially a container for one weight and one bias). We could also create a custom class inheriting `nn.Module`, but this is convenient for a single-layer model.\n",
    "- `optimizer = optim.SGD(model.parameters(), lr=0.01)` tells PyTorch we want to use SGD to adjust `model.parameters()` (which are $w$ and $b$ of the linear layer) with a 0.01 learning rate.\n",
    "- In the loop: `model(X_train)` produces predictions; `loss_fn(y_pred, y_train)` computes the MSE; `loss.backward()` computes gradients; `optimizer.step()` applies the gradient descent update to the parameters. PyTorch‚Äôs autograd takes care of computing the same gradients we derived manually.\n",
    "\n",
    "After training, you should find `model.weight` and `model.bias` are very close to the values we got before (and close to 5 and 50). The final loss will also match what we saw with manual training. This confirms that using PyTorch‚Äôs built-in methods achieves the same result.\n",
    "\n",
    "One advantage of using the high-level API is that it‚Äôs easy to swap different optimizers or even build more complex models with minimal code change. For example, you could replace `optim.SGD` with `optim.Adam` (a more advanced optimizer) and it would work similarly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba87f7",
   "metadata": {},
   "source": [
    "Now let's try a larger learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple linear regression model using nn.Linear\n",
    "model = nn.Linear(1, 1)  # 1 input feature, 1 output feature\n",
    "\n",
    "# Define the Mean Squared Error loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_log = []\n",
    "\n",
    "# Define an optimizer (Stochastic Gradient Descent) to update our model parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare data shape: currently hours and scores are shape [N], we need [N,1] for model\n",
    "X_train = hours.view(-1, 1)   # reshape to (100, 1)\n",
    "y_train = scores.view(-1, 1)\n",
    "\n",
    "# Training loop (similar structure, but using PyTorch machinery)\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: compute predictions\n",
    "    y_pred = model(X_train)               # model does: w*x + b internally\n",
    "    loss = loss_fn(y_pred, y_train)      # compute MSE loss between predictions and true scores\n",
    "    loss_log.append(loss.detach().numpy())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Backward pass and optimization step\n",
    "    optimizer.zero_grad()   # reset gradients from previous step\n",
    "    loss.backward()         # compute gradients of loss w.rt. model parameters\n",
    "    optimizer.step()        # update parameters (w and b) using those gradients\n",
    "\n",
    "# After training, examine the learned parameters\n",
    "w_hat = model.weight.item()\n",
    "b_hat = model.bias.item()\n",
    "print(f\"Model learned w = {w_hat:.3f}, b = {b_hat:.3f}\")\n",
    "print(f\"Final loss = {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e27ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the learned line, and the loss curve\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Left: scatter plot of data and fitted line\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(hours.numpy(), scores.numpy(), color='blue', label='Data')\n",
    "# plot the learned line:\n",
    "x_line = torch.tensor([0.0, 10.0])  # two end-points (min and max hours)\n",
    "y_line = w_hat * x_line + b_hat\n",
    "plt.plot(x_line.numpy(), y_line.detach().numpy(), color='red', label=f'Fit: y={w_hat:.1f}x+{b_hat:.1f}')\n",
    "plt.xlabel(\"Hours Studied\")\n",
    "plt.ylabel(\"Exam Score\")\n",
    "plt.title(\"Data and Fitted Line\")\n",
    "plt.legend()\n",
    "\n",
    "# Right: loss curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(epochs), loss_log, color='purple')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0ba5c",
   "metadata": {},
   "source": [
    "## 2.6 Discussion and Next Steps\n",
    "\n",
    "Congratulations ‚Äì you have implemented and trained a linear regression model using two approaches! üéâ  \n",
    "This simple exercise illustrated the core ideas of machine learning training: we defined a model with parameters, chose a loss function (MSE), computed gradients, and updated the parameters to reduce the loss. This is the essence of training any machine learning model, from linear regression to deep neural networks.\n",
    "\n",
    "A few takeaways and things to keep in mind:\n",
    "\n",
    "- **Model complexity:** Linear regression is a very simple, *linear* model. It can capture linear trends but cannot fit more complex patterns (for instance, if the true relationship is quadratic or otherwise nonlinear). Many real-world problems aren‚Äôt linear, which is why we move to more complex models (like neural networks) in later weeks.\n",
    "\n",
    "- **Over/Under-shooting:** We briefly mentioned learning rate. If your training diverged or the loss didn‚Äôt go down, the learning rate might have been too high (causing overshooting). If it was extremely slow, maybe it was too low. This hyperparameter often needs tuning.\n",
    "\n",
    "- **Autograd magic:** We relied on autograd for the second part. It‚Äôs good to know that under the hood, PyTorch built a computational graph of our model and computed gradients using the chain rule, just like we did manually. For complex models, autograd is indispensable.\n",
    "\n",
    "- **No Free Lunch:** Our final linear model didn‚Äôt fit every point exactly (due to noise). Also, real data can have outliers or non-linear patterns that linear regression won‚Äôt handle well. In practice, we consider more robust models or data preprocessing for such cases. But linear regression is still a fundamental starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631df4ce",
   "metadata": {},
   "source": [
    "**Great job on completing Week 1!** You‚Äôve laid the groundwork in PyTorch and linear models, which we will build upon in the coming weeks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
