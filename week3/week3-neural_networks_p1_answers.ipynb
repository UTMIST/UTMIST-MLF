{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLF Week 3: Neural Networks Part 1 - Foundations\n",
    "\n",
    "The notebook accompanies the **MLF Week 3 Slides**, with a focus on the foundations of neural networks, including **perceptron and single-layer networks**, **activation functions**, **forward propagation**, and **loss functions**, and their corresponding implementation in PyTorch.  \n",
    "\n",
    "**You will:**\n",
    "- Define simple PyTorch models using **torch.nn.Linear**, and activation functions (**torch.nn.ReLU**, **torch.nn.tanh**)\n",
    "- Manually compute forward passes\n",
    "- Implement a simple 2-hidden-layer network to solve a basic classification task\n",
    "- Plot decision boundaries and predictions\n",
    "- Build and train an MLP for classification using the MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the necessary libraries\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84cb908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries that were installed \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df049a",
   "metadata": {},
   "source": [
    "# Building Neural Networks\n",
    "\n",
    "Using PyTorch, we can build neural networks in a layer-by-layer manner. These layers each perform operations on our input data. The **torch.nn** package provides all the necessary building blocks to build any neural network.\n",
    "\n",
    "We define the neural network as **python class** which subclasses the **nn.Module** class. For a simple feed-forward neural network (FFNN), we initalize the layers in **\\__self__** and define a method called **forward** to compute the forward pass. \n",
    "\n",
    "Note: the function that computes the forward pass **must be named forward** otherwise the nn.Module class will not know how to use it to compute the forward pass. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961819e",
   "metadata": {},
   "source": [
    "The following is an example of a simple neural network that consists of 1 input layer, 1 hidden layer, and 1 output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1de1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=2, out_features=5) #input_layer dim = 2, hidden_layer dim = 5\n",
    "        self.relu = nn.ReLU #ReLU activation function\n",
    "        self.layer2 = nn.Linear(in_features=5, out_features=1) #hidden_layer dim = 5, output_layer dim = 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x) #data passes from input to hidden layer\n",
    "        x = self.relu(x) #relu activation function is applied to output of hidden layer\n",
    "        x = self.layer2(x) #data passes from hidden layer to output layer\n",
    "        return x #output from hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25796f3",
   "metadata": {},
   "source": [
    "## Exercise 1: Now it's your turn!\n",
    "\n",
    "TODO: Define and build a simple PyTorch neural network model using torch.nn.Linear, torch.nn.ReLU, torch.nn.tanh:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ed137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A sample solution could be:\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=2, out_features=8) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(in_features=8, out_features=16) \n",
    "        self.layer3 = nn.Linear(in_features=16, out_features=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e01fa",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "As a reminder, forward propagation refers to how input data flows the the network and is transformed by each layer it flows through.\n",
    "\n",
    "Forward propagation can a given layer $l$ can be represented using the following formula: \n",
    "\\begin{equation}\n",
    "\\mathbf{z}^{[l]} = \\phi^{[l]}\\left(\\mathbf{W}^{[l]} \\mathbf{x}^{[l-1]} + \\mathbf{b}^{[l]}  \\right)\n",
    "\\end{equation}\n",
    "Where:\n",
    "- $ \\mathbf{x}^{[l-1]} $ is the input to given layer $l$ / post-activation output from the previous layer $l-1$\n",
    "- $ \\mathbf{W}^{[l]} $ and $ \\mathbf{b}^{[l]} $ are the weights and biases at layer $ l $\n",
    "- $ \\phi^{[l]} $ is the activation function at layer $ l $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790fb67",
   "metadata": {},
   "source": [
    "## Exercise 2: Manually compute a forward pass for a simple neural network\n",
    "\n",
    "TODO: Complete the function that manually computes the forward pass for a simple neural network consisting of 1 hidden layer. For the activation function, use ReLU. Do not apply an activation fucntion to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_forward_pass(x, w1, w2, b1, b2):\n",
    "    #TODO: Complete the function\n",
    "    z1 = w1 @ x + b1\n",
    "    h1 = F.relu(z1)\n",
    "    z2 = w2 @ h1 + b2\n",
    "    y = F.relu(z2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73ac83",
   "metadata": {},
   "source": [
    "To check if your implementation of the manual_forward_pass function is correct, please run the following function that will grade your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import test_exercise_2, show_result\n",
    "\n",
    "res = test_exercise_2(manual_forward_pass)\n",
    "show_result(\"Exercise 2\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2118901e",
   "metadata": {},
   "source": [
    "## Exercise 3: Basic classification task\n",
    "\n",
    "In this exercise you will build a simple 2-hidden-layer neural network to solve a basic classification task. The classification task will be to classify data points that are grouped into two concentric circles, similar to what you have seen in this week's presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ffc1c",
   "metadata": {},
   "source": [
    " ## 3.1 Generate dataset:\n",
    "\n",
    " The dataset is generated and can be visualized using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8037c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the following cell to create the dataset\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=2000, factor=0.2, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the following cell to visualize the data\n",
    "\n",
    "X_class0 = X[y == 0]\n",
    "X_class1 = X[y == 1]\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_class0[:, 0], X_class0[:, 1], color='blue', label='Class 0', edgecolor='k', s=40)\n",
    "plt.scatter(X_class1[:, 0], X_class1[:, 1], color='red', label='Class 1', edgecolor='k', s=40)\n",
    "plt.title(\"2D Circle Classification Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0123bf9",
   "metadata": {},
   "source": [
    "## 3.2 Perform a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split on data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1fd75",
   "metadata": {},
   "source": [
    "## 3.3 Design Neural Network\n",
    "\n",
    "TODO: Design a neural network called FFNN that has 2 hidden layers that performs binary classification on the dataset we created earlier\n",
    "\n",
    "Note: Since we are performing a binary classification task, we should have a sigmoid activation function at the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0bdc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    #TODO: Build a model that can perform this classification task\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #An example solution could be\n",
    "        self.layer1 = nn.Linear(2, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(16, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f8dc7",
   "metadata": {},
   "source": [
    "## 3.4 Instantiate and train the model\n",
    "\n",
    "The code below instantiates and trains the model. Since we are performing binary classification, the loss function we use should be binary cross-entropy loss. In addition, we are also using Adam as an optimizer to learn the neural network weights and biases. \n",
    "\n",
    "Feel free to change the number of epochs to see how it changes the model accuracy.\n",
    "\n",
    "Do not worry if you are struggling to understand the code, you will learn how neural networks are trained to learn weights and biases through backpropagation in the session next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "      preds = (outputs >= 0.5).int()\n",
    "      correct = (preds == y_train).sum().item()\n",
    "      total = y_train.size(0)\n",
    "      accuracy = correct / total\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs}, Loss: {loss.item():.4f}, Accuracy:{accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d9a9e",
   "metadata": {},
   "source": [
    "## 3.5 Visualizing decision boundary and predictions\n",
    "\n",
    "Run the cell below to see how the neural network classifies data points in the test dataset and visualizes the decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_min, x_max = X_test[:, 0].min() - 0.5, X_test[:, 0].max() + 0.5\n",
    "y_min, y_max = X_test[:, 1].min() - 0.5, X_test[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                      np.linspace(y_min, y_max, 300))\n",
    "\n",
    "#flatten and create grid tensor\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_tensor = torch.FloatTensor(grid)\n",
    "\n",
    "#model looks at every point on the grid and classifies them as class 0 or class 1 based on what it had learnt\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    probs = model(grid_tensor).reshape(xx.shape).numpy()\n",
    "\n",
    "#model runs predictions on test data\n",
    "with torch.no_grad():\n",
    "    test_probs = model(X_test)\n",
    "    test_preds = (test_probs >= 0.5).int().squeeze().numpy()\n",
    "correct = (test_preds == y_test.squeeze())\n",
    "\n",
    "#plot decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, probs, levels=[0, 0.5, 1], cmap='RdBu', alpha=0.6)\n",
    "\n",
    "#plot correct predictions (green), incorrect (red)\n",
    "plt.scatter(X_test[correct, 0], X_test[correct, 1], c='green', label='Correct', edgecolors='k', s=40)\n",
    "plt.scatter(X_test[~correct, 0], X_test[~correct, 1], c='red', label='Incorrect', edgecolors='k', s=40)\n",
    "\n",
    "plt.title(\"Decision Boundary with Test Predictions\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e9e5e",
   "metadata": {},
   "source": [
    "## Exercise 4: Build and train an MLP for the MNIST dataset\n",
    "\n",
    "The MNIST dataset consists of 28x28 dimensional images (i.e 2D tensors) of different handwritten digits. In this exercise, you will build a neural network that trains and performs multi-class classification on a more robust dataset. Feel free to experiment with any amount of hidden layers and nodes in your implementation.\n",
    "\n",
    "Note: Since the input data is 2D, you will need to flatten the input to make it a 1D vector to feed into the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3cf48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libarries for dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba7c97",
   "metadata": {},
   "source": [
    "## 4.1 Prepare the MNIST data by creating the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67617b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data and performing train-test split\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "num_output_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256396e1",
   "metadata": {},
   "source": [
    "## 4.2 Designing the neural network model\n",
    "\n",
    "TODO: Design a neural network model to perform multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNN(nn.Module):\n",
    "    #TODO: Define the MLP Model\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #An example solution could be:\n",
    "        self.flatten = nn.Flatten() \n",
    "        self.layer1 = nn.Linear(28*28, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, num_output_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42e673",
   "metadata": {},
   "source": [
    "## 4.1 Instantiate and train the model\n",
    "\n",
    "It may be better to do this exercise in Google Colab to use its GPU capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4eed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        accuracy = correct / total\n",
    "    print(f\"{epoch+1}/{epochs}, Loss={train_loss/len(train_loader)}, Accuracy={accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f8772",
   "metadata": {},
   "source": [
    "## 4.2 Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea786d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Test accuracy {100 * correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4053b",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "\n",
    "You now have:\n",
    "- Built a simple neural network using torch.nn.Linear layers and various activation functions\n",
    "- Manually computed forward passes between layers of a neural network\n",
    "- Used neural networks to perform classification tasks\n",
    "\n",
    "**Great job on completing Week 3!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
