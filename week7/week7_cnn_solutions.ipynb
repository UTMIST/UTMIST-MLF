{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ad7ec0",
   "metadata": {},
   "source": [
    "# Week 7 â€” Convolutional Neural Networks (Image Classification) â€” **Solutions**\n",
    "\n",
    "This notebook contains **one possible set of solutions** for the Week 7 lab.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load the **CIFARâ€‘10** dataset using ðŸ¤— `datasets` and prepare PyTorch `DataLoader`s.\n",
    "- Build a **PCA + Logistic Regression** baseline and evaluate it (accuracy, precision, recall, F1, confusion matrix).\n",
    "- Implement a **simple CNN** that barely beats (or even underperforms) the baseline.\n",
    "- Implement a **stronger CNN** that clearly outperforms the baseline.\n",
    "- Train a CNN on **dataâ€‘augmented images** and inspect augmentations.\n",
    "- Add an **advanced CNN feature (Global Average Pooling)** and compare its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4064707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92672446",
   "metadata": {},
   "source": [
    "## 1. Loading CIFARâ€‘10 and Visualizing Samples\n",
    "\n",
    "We will use the **CIFARâ€‘10** dataset from ðŸ¤— `datasets`.  \n",
    "Each example is a `32Ã—32` RGB image from one of 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFARâ€‘10 from Hugging Face Datasets\n",
    "cifar10 = load_dataset(\"cifar10\")\n",
    "\n",
    "# For faster experiments, we can optionally subsample\n",
    "MAX_TRAIN = 20000   # out of 50k\n",
    "MAX_TEST = 4000     # out of 10k\n",
    "\n",
    "train_split = cifar10[\"train\"].shuffle(seed=42).select(range(MAX_TRAIN))\n",
    "test_split = cifar10[\"test\"].shuffle(seed=42).select(range(MAX_TEST))\n",
    "\n",
    "class_labels = cifar10[\"train\"].features[\"label\"].names\n",
    "print(\"Classes:\", class_labels)\n",
    "\n",
    "# Standard normalization values for CIFARâ€‘10\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2470, 0.2435, 0.2616]\n",
    "\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "class HFCIFAR10(Dataset):\n",
    "    def __init__(self, hf_split, transform=None):\n",
    "        self.hf_split = hf_split\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_split)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.hf_split[idx]\n",
    "        img = example[\"img\"]  # PIL Image\n",
    "        label = example[\"label\"]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "train_dataset = HFCIFAR10(train_split, transform=basic_transform)\n",
    "test_dataset = HFCIFAR10(test_split, transform=basic_transform)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec972dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch of training images (unnormalized for display)\n",
    "inv_transform = transforms.Normalize(\n",
    "    mean=[-m/s for m, s in zip(mean, std)],\n",
    "    std=[1/s for s in std],\n",
    ")\n",
    "\n",
    "def show_batch(loader, n_images=16):\n",
    "    images, labels = next(iter(loader))\n",
    "    images = images[:n_images]\n",
    "    labels = labels[:n_images]\n",
    "\n",
    "    images = inv_transform(images)  # roughly undo normalization\n",
    "\n",
    "    grid_size = int(math.sqrt(n_images))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(6, 6))\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i >= n_images:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(class_labels[labels[i].item()], fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_batch(train_loader, n_images=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede26be0",
   "metadata": {},
   "source": [
    "## 2. PCA + Logistic Regression Baseline (Solution)\n",
    "\n",
    "To build a strong but simple baseline:\n",
    "\n",
    "1. **Flatten** each image to a vector of size `32Ã—32Ã—3 = 3072`.\n",
    "2. Fit **PCA** on training vectors (e.g., 100 components).\n",
    "3. Train **Logistic Regression** on the PCA features.\n",
    "4. Evaluate using accuracy, precision, recall, F1 and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9162250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_true, y_pred):\n",
    "    \"\"\"Compute and print accuracy, macro precision/recall/F1, and confusion matrix.\"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1â€‘score : {f1:.4f}\")\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(cm)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"cm\": cm,\n",
    "    }\n",
    "\n",
    "\n",
    "def pca_logistic_baseline(train_dataset, test_dataset, n_components=100, max_iter=1000):\n",
    "    # Convert datasets to numpy arrays\n",
    "    def dataset_to_numpy(ds):\n",
    "        X_list, y_list = [], []\n",
    "        for img, y in DataLoader(ds, batch_size=256):\n",
    "            # undo normalization for PCA stability (optional)\n",
    "            img = inv_transform(img)\n",
    "            X_list.append(img.view(img.size(0), -1).numpy())\n",
    "            y_list.append(y.numpy())\n",
    "        X = np.concatenate(X_list, axis=0)\n",
    "        y = np.concatenate(y_list, axis=0)\n",
    "        return X, y\n",
    "\n",
    "    X_train, y_train = dataset_to_numpy(train_dataset)\n",
    "    X_test, y_test = dataset_to_numpy(test_dataset)\n",
    "\n",
    "    print(\"Train X shape:\", X_train.shape)\n",
    "    print(\"Test  X shape:\", X_test.shape)\n",
    "\n",
    "    # 1) Fit PCA\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # 2) Fit Logistic Regression on PCA features\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=max_iter,\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "\n",
    "    # 3) Evaluate\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    metrics = evaluate_classification(y_test, y_pred)\n",
    "    return metrics\n",
    "\n",
    "pca_metrics = pca_logistic_baseline(train_dataset, test_dataset, n_components=100)\n",
    "print(\"\\nPCA + Logistic Regression baseline accuracy:\", pca_metrics[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6d7d2",
   "metadata": {},
   "source": [
    "## 3. Simple CNN (Solution)\n",
    "\n",
    "We now implement a small CNN that may **struggle** to significantly beat the baseline:\n",
    "\n",
    "- Two convolutional layers with small numbers of filters.\n",
    "- A small fullyâ€‘connected head.\n",
    "- Trained for only a few epochs.\n",
    "\n",
    "This is intentionally *underâ€‘powered* so we can appreciate why a good baseline is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce35afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # 32x32 -> 16x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # 16x16 -> 8x8\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion=None):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            if criterion is not None:\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                total += labels.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    metrics = evaluate_classification(all_labels, all_preds)\n",
    "    if criterion is not None and total > 0:\n",
    "        metrics[\"loss\"] = running_loss / total\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=5, lr=1e-3):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | train_acc={train_acc:.4f}\")\n",
    "\n",
    "    print(\"\\nFinal evaluation on test set:\")\n",
    "    test_metrics = evaluate_model(model, test_loader, criterion)\n",
    "    print(\"Test accuracy:\", test_metrics[\"accuracy\"])\n",
    "    return test_metrics\n",
    "\n",
    "simple_cnn = SimpleCNN(num_classes=len(class_labels))\n",
    "simple_metrics = train_model(simple_cnn, train_loader, test_loader, epochs=3, lr=1e-3)\n",
    "print(\"\\nSimple CNN test accuracy:\", simple_metrics[\"accuracy\"], \"(baseline was ~\", pca_metrics[\"accuracy\"], \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625ee87",
   "metadata": {},
   "source": [
    "## 4. Stronger CNN (Solution)\n",
    "\n",
    "Now we build a **deeper CNN** with more channels and layers:\n",
    "\n",
    "- 3 convolutional blocks (Conv â†’ ReLU â†’ BatchNorm â†’ MaxPool).\n",
    "- A larger fullyâ€‘connected head with dropout.\n",
    "\n",
    "We train it for more epochs, expecting better performance than both the PCA baseline and the simple CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProperCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),   # 32 -> 16\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),   # 16 -> 8\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),   # 8 -> 4\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "proper_cnn = ProperCNN(num_classes=len(class_labels))\n",
    "proper_metrics = train_model(proper_cnn, train_loader, test_loader, epochs=8, lr=1e-3)\n",
    "print(\"\\nProper CNN test accuracy:\", proper_metrics[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec71a8e",
   "metadata": {},
   "source": [
    "## 5. CNN with Data Augmentation (Solution)\n",
    "\n",
    "To improve generalization we apply common **data augmentation** techniques:\n",
    "\n",
    "- Random horizontal flips.\n",
    "- Random crops with padding.\n",
    "\n",
    "We keep the test transform unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f54606",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "aug_train_dataset = HFCIFAR10(train_split, transform=augment_transform)\n",
    "aug_train_loader = DataLoader(aug_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Original train size:\", len(train_dataset))\n",
    "print(\"Augmented train size (same images, different transforms each epoch):\", len(aug_train_dataset))\n",
    "\n",
    "aug_cnn = ProperCNN(num_classes=len(class_labels))\n",
    "aug_metrics = train_model(aug_cnn, aug_train_loader, test_loader, epochs=8, lr=1e-3)\n",
    "print(\"\\nAugmented CNN test accuracy:\", aug_metrics[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbaba9",
   "metadata": {},
   "source": [
    "## 6. Advanced CNN Feature: Global Average Pooling (Solution)\n",
    "\n",
    "A common architectural pattern in modern CNNs is **Global Average Pooling (GAP)**:\n",
    "\n",
    "- Replace large fullyâ€‘connected layers on top of feature maps with\n",
    "  a global average over spatial dimensions (`HÃ—W â†’ 1Ã—1` per channel).\n",
    "- This reduces the number of parameters and can act as a form of regularization.\n",
    "\n",
    "We build a model that uses GAP before the final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e36dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAPCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))  # global average pooling\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)              # (B, 128, 1, 1)\n",
    "        x = x.view(x.size(0), -1)    # (B, 128)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "gap_cnn = GAPCNN(num_classes=len(class_labels))\n",
    "gap_metrics = train_model(gap_cnn, aug_train_loader, test_loader, epochs=8, lr=1e-3)\n",
    "print(\"\\nGAP CNN test accuracy:\", gap_metrics[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29a6d6",
   "metadata": {},
   "source": [
    "## 7. Discussion â€” Sample Answers\n",
    "\n",
    "**1. Did the deeper CNN outperform the baseline models?**  \n",
    "Yes. In typical runs, the **PCA + Logistic Regression** baseline reaches around *40â€“45%* test accuracy,  \n",
    "the **simple CNN** reaches *similar or slightly better* performance, while the **Proper CNN** and **augmented CNN** usually reach *60â€“75%* depending on hyperâ€‘parameters and runtime. This clearly shows the benefit of more expressive models for image data.\n",
    "\n",
    "**2. How did data augmentation affect performance?**  \n",
    "Data augmentation usually **improves generalization**. Even though the training loss might decrease more slowly, the augmented CNN often gets **higher test accuracy** than the same architecture trained on nonâ€‘augmented images. The model sees more varied versions of the same objects (different crops, flips), so it becomes less sensitive to small perturbations.\n",
    "\n",
    "**3. What effect did Global Average Pooling have?**  \n",
    "Global Average Pooling removes large fullyâ€‘connected layers over spatial maps and replaces them with a simple average. This:\n",
    "- **Reduces parameters**, making the model lighter and less prone to overfitting.\n",
    "- Forces the network to learn more **global, translationâ€‘invariant** features.\n",
    "In practice the GAP model often performs similarly to or slightly better than the nonâ€‘GAP CNN, but with fewer parameters.\n",
    "\n",
    "**4. Why is it important to compare against simple baselines?**  \n",
    "Without a baseline we might celebrate a CNN that reaches, say, 45% accuracy, without realizing that a **simple PCA + Logistic Regression** already gets 40â€“45%. Baselines:\n",
    "- Provide a **sanityâ€‘check** (are we really learning something?).\n",
    "- Help us detect **bugs** or ineffective architectures.\n",
    "- Make it easier to justify the **extra complexity** of deep models when they actually provide gains.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
