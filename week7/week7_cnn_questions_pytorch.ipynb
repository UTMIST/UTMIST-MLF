{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell first if you don't have these packages installed\n",
    "!pip install numpy matplotlib scikit-learn datasets torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 \u2014 Convolutional Neural Networks (Image Classification) - PyTorch Version\n",
    "\n",
    "**Setup Instructions:**\n",
    "1. Run the cell above to install required packages (if not already installed)\n",
    "2. **Important:** If you get import errors after installing packages, restart the Jupyter kernel:\n",
    "   - Go to `Kernel` \u2192 `Restart Kernel` in the menu\n",
    "   - Or use the restart button in the toolbar\n",
    "3. Then re-run the import cell below\n",
    "\n",
    "**Note:** This notebook uses PyTorch instead of NumPy for CNN implementation. PyTorch provides automatic differentiation and GPU support, making it easier to build and train neural networks.\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "- Load and visualize the CIFAR-100 dataset from HuggingFace.\n",
    "- Build a PCA + Logistic Regression baseline classifier; compute evaluation metrics (e.g., accuracy).\n",
    "- Implement a simple CNN classifier using PyTorch that barely beats or fails to beat the baseline.\n",
    "- Build a deeper CNN model that achieves better performance.\n",
    "- Train a CNN model on data\u2011augmented images and visualize augmentations.\n",
    "- Explore an advanced CNN feature (e.g., global average pooling) and observe its impact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import (\n",
    "    show_result,\n",
    "    load_cifar100_dataset,\n",
    "    pca_logistic_baseline,\n",
    "    test_exercise_7_pca,\n",
    "    test_exercise_7_simple_cnn,\n",
    "    test_exercise_7_proper_cnn,\n",
    "    test_exercise_7_data_aug_cnn,\n",
    "    test_exercise_7_advanced_cnn,\n",
    "    accuracy\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CIFAR-100 Image Dataset\n",
    "\n",
    "In this exercise, we'll use the CIFAR-100 dataset, a well-known benchmark dataset for image classification. CIFAR-100 contains 60,000 32\u00d732 color images in 100 classes (100 fine-grained classes), with 6,000 images per class.\n",
    "\n",
    "We'll load the dataset from HuggingFace and convert it to grayscale to simplify training and focus on the CNN architecture rather than computational complexity.\n",
    "\n",
    "**Task:** Use the `load_cifar10_dataset` function from `utils.py` to load a subset of CIFAR-100, then visualize a few random samples from each class. Report the number of training and test examples.\n",
    "\n",
    "**Hint:** Call `load_cifar10_dataset(n_train=50000, n_test=200, seed=0, grayscale=False)` to get a manageable subset. The function returns 5 values: train images, train labels, test images, test labels, and class names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 dataset\n",
    "# Using the full dataset for high performance\n",
    "X_train, y_train, X_test, y_test, class_names = load_cifar100_dataset(\n",
    "    n_train=50000, n_test=10000, seed=0, grayscale=False\n",
    ")\n",
    "print(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
    "print(f\"Image shape: {X_train.shape[1:]}\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Visualize a few random samples from the training set\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 2.5))\n",
    "for ax in axes:\n",
    "    idx = random.randint(0, len(X_train) - 1)\n",
    "    ax.imshow(X_train[idx]) # RGB, no cmap needed\n",
    "    ax.set_title(f\"{class_names[y_train[idx]]}\\n(label {y_train[idx]})\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA + Logistic Regression Baseline\n",
    "\n",
    "A simple yet strong baseline for image classification is to flatten each image into a vector, project it onto a lower\u2011dimensional subspace using **Principal Component Analysis (PCA)**, and then train a multinomial logistic regression classifier.\n",
    "\n",
    "1. Flatten the training and test images (shape `(N, H*W)`).\n",
    "2. Fit a PCA model on the training data and project both the training and test data into a lower\u2011dimensional space (e.g., 20 components).\n",
    "3. Train a `LogisticRegression` classifier on the reduced features.\n",
    "4. Evaluate the classifier using **accuracy** (the fraction of correct predictions).\n",
    "\n",
    "**Task:** Complete the function `student_pca_baseline(...)` below to implement this baseline. It should return the test accuracy as a float in `[0,1]`.\n",
    "\n",
    "**Hints:**\n",
    "- Use `train_images.reshape(train_images.shape[0], -1)` to flatten images from (N, H, W) to (N, H*W)\n",
    "- Import `PCA` from `sklearn.decomposition` and `LogisticRegression` from `sklearn.linear_model`\n",
    "- Make sure `n_components` doesn't exceed the number of features (use `min(n_components, n_features)`)\n",
    "- Use `pca.fit_transform()` for training data and `pca.transform()` for test data\n",
    "- The `accuracy` function from `utils` computes the classification accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_pca_baseline(train_images, train_labels, test_images, test_labels, n_components=20):\n",
    "    '''\n",
    "    Implements a PCA + Logistic Regression baseline classifier.\n",
    "\n",
    "    Parameters:\n",
    "        train_images: numpy array of shape (N_train, H, W) with float32 values in [0,1].\n",
    "        train_labels: numpy array of shape (N_train,) of integer labels.\n",
    "        test_images: numpy array of shape (N_test, H, W).\n",
    "        test_labels: numpy array of shape (N_test,).\n",
    "        n_components: number of principal components to retain.\n",
    "\n",
    "    Returns:\n",
    "        Test accuracy as a float in [0,1].\n",
    "    '''\n",
    "    # TODO: flatten images, fit PCA, train LogisticRegression, compute accuracy\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the PCA baseline implementation\n",
    "res = test_exercise_7_pca(student_pca_baseline)\n",
    "show_result(\"Exercise 1 \u2013 PCA Baseline\", res)\n",
    "\n",
    "# If implemented, you can also test on the dataset generated above\n",
    "try:\n",
    "    acc = student_pca_baseline(X_train, y_train, X_test, y_test, 20)\n",
    "    print(f\"PCA baseline accuracy on the synthetic dataset: {acc:.3f}\")\n",
    "except NotImplementedError:\n",
    "    print(\"Implement student_pca_baseline above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Convolutional Neural Network (PyTorch)\n",
    "\n",
    "Convolutional neural networks (CNNs) process images by learning **filters** that extract local patterns. We'll start with a very small CNN using PyTorch:\n",
    "\n",
    "- One convolutional layer with a few filters (e.g., 4 filters, each $3\\times3$).\n",
    "- Apply a non\u2011linear activation such as ReLU.\n",
    "- Flatten the result and feed it into a linear layer to produce class logits.\n",
    "\n",
    "For training, use cross\u2011entropy loss and an optimizer (e.g., SGD) for a few epochs. Because this network is very shallow and the dataset is small, it may perform worse than the PCA baseline.\n",
    "\n",
    "**Task:** Complete the function `student_simple_cnn(...)` below. It should construct the described network using PyTorch's `nn.Module`, train it for a few epochs on the training set, and return the test accuracy.\n",
    "\n",
    "**Hints:**\n",
    "- Use `nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3)` for the convolutional layer\n",
    "- Use `nn.ReLU()` for activation\n",
    "- Use `nn.Flatten()` or `.view()` to flatten the feature maps\n",
    "- Use `nn.Linear()` for the final classification layer\n",
    "- Convert NumPy arrays to PyTorch tensors: `torch.from_numpy(arr).float()`\n",
    "- Move tensors to device: `.to(device)`\n",
    "- Use `nn.CrossEntropyLoss()` for loss and `optim.SGD()` for optimizer\n",
    "- Remember to call `model.train()` during training and `model.eval()` during evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_simple_cnn(train_images, train_labels, test_images, test_labels, num_epochs=5, learning_rate=0.01):\n",
    "    '''\n",
    "    Build and train a simple CNN with one convolutional layer followed by a linear classifier using PyTorch.\n",
    "    Use small filter sizes (e.g., 3x3) and a small number of filters (e.g., 4).\n",
    "\n",
    "    Parameters:\n",
    "        train_images: numpy array (N_train, H, W).\n",
    "        train_labels: numpy array (N_train,).\n",
    "        test_images: numpy array (N_test, H, W).\n",
    "        test_labels: numpy array (N_test,).\n",
    "        num_epochs: number of training epochs.\n",
    "        learning_rate: step size for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        Test accuracy as a float.\n",
    "    '''\n",
    "    # TODO: implement PyTorch CNN model, training loop, and evaluation\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the simple CNN implementation\n",
    "res = test_exercise_7_simple_cnn(student_simple_cnn)\n",
    "show_result(\"Exercise 2 \u2013 Simple CNN\", res)\n",
    "\n",
    "# Optional: test on the dataset generated above\n",
    "try:\n",
    "    acc = student_simple_cnn(X_train, y_train, X_test, y_test)\n",
    "    print(f\"Simple CNN accuracy: {acc:.3f}\")\n",
    "except NotImplementedError:\n",
    "    print(\"Implement student_simple_cnn above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deeper CNN (Improved Model)\n",
    "\n",
    "Now extend your network to be **deeper** with multiple convolutional blocks. Your goal is to achieve **>50% accuracy on CIFAR-100**.\n",
    "\n",
    "**Suggested Architecture** (you can modify this):\n",
    "- **Block 1:** [Conv(3\u219264) \u2192 BN \u2192 ReLU \u2192 Conv(64\u219264) \u2192 BN \u2192 ReLU] \u2192 MaxPool(2\u00d72) \u2192 Dropout(0.1)\n",
    "- **Block 2:** [Conv(64\u2192128) \u2192 BN \u2192 ReLU \u2192 Conv(128\u2192128) \u2192 BN \u2192 ReLU] \u2192 MaxPool(2\u00d72) \u2192 Dropout(0.2)\n",
    "- **Block 3:** [Conv(128\u2192256) \u2192 BN \u2192 ReLU \u2192 Conv(256\u2192256) \u2192 BN \u2192 ReLU] \u2192 MaxPool(2\u00d72) \u2192 Dropout(0.3)\n",
    "- **Classifier:** Flatten \u2192 Linear(256\u00d74\u00d74 \u2192 512) \u2192 BN \u2192 ReLU \u2192 Dropout(0.5) \u2192 Linear(512 \u2192 100)\n",
    "\n",
    "**Key Components:**\n",
    "1. **Batch Normalization** (`nn.BatchNorm2d`): Normalizes activations, speeds up training\n",
    "2. **Dropout** (`nn.Dropout2d` for conv layers, `nn.Dropout` for fc layers): Prevents overfitting\n",
    "3. **Data Augmentation**: Apply random transformations during training\n",
    "4. **Mini-batch Training**: Use `DataLoader` to train in batches\n",
    "\n",
    "**Task:** Complete the function `student_proper_cnn(...)` below.\n",
    "\n",
    "**Hints:**\n",
    "- Use `padding=1` in `Conv2d` to preserve spatial dimensions (32\u00d732 \u2192 32\u00d732)\n",
    "- Use `nn.MaxPool2d(2, 2)` to downsample by 2x (32\u00d732 \u2192 16\u00d716 \u2192 8\u00d78 \u2192 4\u00d74)\n",
    "- After 3 pooling layers: 32\u00f72\u00f72\u00f72 = 4, so final feature map is 4\u00d74\n",
    "- Progressive dropout: 0.1 \u2192 0.2 \u2192 0.3 \u2192 0.5 (less in early layers, more in later layers)\n",
    "- **Data Augmentation** (apply during training):\n",
    "  - Random horizontal flip: `torch.flip(img, dims=[3])`\n",
    "  - Random crop: `F.pad(img, (4,4,4,4), mode='reflect')` then crop 32\u00d732\n",
    "- **Weight Initialization**: Use Kaiming (He) initialization for ReLU networks\n",
    "  ```python\n",
    "  nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "  ```\n",
    "- **Optimizer**: `optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)`\n",
    "- **Scheduler**: `CosineAnnealingLR` for smooth learning rate decay\n",
    "- **Loss**: `nn.CrossEntropyLoss(label_smoothing=0.1)` prevents overconfidence\n",
    "- **Gradient Clipping**: `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)`\n",
    "- **Training**: 50 epochs with batch_size=64 should achieve >50% accuracy\n",
    "- **Mini-batches**: Use `TensorDataset` and `DataLoader` for efficient batch training\n",
    "\n",
    "**Example Structure:**\n",
    "```python\n",
    "class ProperCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProperCNN, self).__init__()\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.1)\n",
    "        # ... continue with blocks 2, 3, and classifier\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Guide\n",
    "\n",
    "**What's PROVIDED for you:**\n",
    "- \u2705 Complete `ProperCNN` model architecture (3 conv blocks + classifier)\n",
    "- \u2705 `augment_batch()` function for data augmentation\n",
    "- \u2705 Data preparation (tensors, DataLoader)\n",
    "- \u2705 Model initialization with Kaiming weights\n",
    "- \u2705 Loss function, optimizer, and scheduler setup\n",
    "\n",
    "**What YOU need to implement:**\n",
    "- \ud83d\udd28 **Training Loop**: Iterate over epochs and batches, apply augmentation, forward/backward pass, gradient clipping\n",
    "- \ud83d\udd28 **Evaluation**: Set model to eval mode, make predictions on test set, compute accuracy\n",
    "\n",
    "**Your tasks are clearly marked with `# TODO:` and `# YOUR CODE HERE` comments in the function below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_proper_cnn(train_images, train_labels, test_images, test_labels, num_epochs=50, learning_rate=0.001, batch_size=64):\n",
    "    '''\n",
    "    Build and train a moderately deep CNN optimized for CIFAR-100.\n",
    "    Goal: Achieve >50% test accuracy.\n",
    "    '''\n",
    "    n_classes = len(np.unique(train_labels))\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    \n",
    "    print(f\"Training on {len(train_images)} samples with batch size {batch_size} for {num_epochs} epochs\")\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # MODEL ARCHITECTURE (PROVIDED)\n",
    "    # ============================================================================\n",
    "    class ProperCNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ProperCNN, self).__init__()\n",
    "            # Block 1: 3 -> 64 -> 64\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "            self.pool1 = nn.MaxPool2d(2, 2)  # 32x32 -> 16x16\n",
    "            self.dropout1 = nn.Dropout2d(0.1)\n",
    "            \n",
    "            # Block 2: 64 -> 128 -> 128\n",
    "            self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "            self.bn3 = nn.BatchNorm2d(128)\n",
    "            self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "            self.bn4 = nn.BatchNorm2d(128)\n",
    "            self.pool2 = nn.MaxPool2d(2, 2)  # 16x16 -> 8x8\n",
    "            self.dropout2 = nn.Dropout2d(0.2)\n",
    "            \n",
    "            # Block 3: 128 -> 256 -> 256\n",
    "            self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "            self.bn5 = nn.BatchNorm2d(256)\n",
    "            self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "            self.bn6 = nn.BatchNorm2d(256)\n",
    "            self.pool3 = nn.MaxPool2d(2, 2)  # 8x8 -> 4x4\n",
    "            self.dropout3 = nn.Dropout2d(0.3)\n",
    "            \n",
    "            # Classifier\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "            self.bn_fc = nn.BatchNorm1d(512)\n",
    "            self.dropout4 = nn.Dropout(0.5)\n",
    "            self.fc2 = nn.Linear(512, n_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Block 1\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            x = F.relu(self.bn2(self.conv2(x)))\n",
    "            x = self.pool1(x)\n",
    "            x = self.dropout1(x)\n",
    "            \n",
    "            # Block 2\n",
    "            x = F.relu(self.bn3(self.conv3(x)))\n",
    "            x = F.relu(self.bn4(self.conv4(x)))\n",
    "            x = self.pool2(x)\n",
    "            x = self.dropout2(x)\n",
    "            \n",
    "            # Block 3\n",
    "            x = F.relu(self.bn5(self.conv5(x)))\n",
    "            x = F.relu(self.bn6(self.conv6(x)))\n",
    "            x = self.pool3(x)\n",
    "            x = self.dropout3(x)\n",
    "            \n",
    "            # Classifier\n",
    "            x = self.flatten(x)\n",
    "            x = F.relu(self.bn_fc(self.fc1(x)))\n",
    "            x = self.dropout4(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "    \n",
    "    # ============================================================================\n",
    "    # DATA AUGMENTATION FUNCTION (PROVIDED)\n",
    "    # ============================================================================\n",
    "    def augment_batch(images_tensor):\n",
    "        '''\n",
    "        Apply random horizontal flip and random crop to a batch of images.\n",
    "        '''\n",
    "        aug = images_tensor.clone()\n",
    "        batch_size = aug.shape[0]\n",
    "        \n",
    "        # Random horizontal flip (50% probability)\n",
    "        flip_mask = torch.rand(batch_size, device=aug.device) > 0.5\n",
    "        aug[flip_mask] = torch.flip(aug[flip_mask], dims=[3])\n",
    "        \n",
    "        # Random crop with padding (shift by up to 4 pixels)\n",
    "        padded = F.pad(aug, (4, 4, 4, 4), mode='reflect')\n",
    "        for i in range(batch_size):\n",
    "            h_start = torch.randint(0, 9, (1,)).item()\n",
    "            w_start = torch.randint(0, 9, (1,)).item()\n",
    "            aug[i] = padded[i, :, h_start:h_start+32, w_start:w_start+32]\n",
    "        \n",
    "        return aug\n",
    "    \n",
    "    # ============================================================================\n",
    "    # DATA PREPARATION (PROVIDED)\n",
    "    # ============================================================================\n",
    "    # Convert numpy arrays to PyTorch tensors and move to device\n",
    "    X_train_tensor = torch.from_numpy(train_images).float().permute(0, 3, 1, 2).to(device)\n",
    "    y_train_tensor = torch.from_numpy(train_labels).long().to(device)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(test_images).float().permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # MODEL INITIALIZATION (PROVIDED)\n",
    "    # ============================================================================\n",
    "    model = ProperCNN().to(device)\n",
    "    \n",
    "    # Initialize weights using Kaiming initialization\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    # Define loss function, optimizer, and scheduler\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # TODO: TRAINING LOOP (YOUR TASK)\n",
    "    # ============================================================================\n",
    "    # Implement the training loop here. Follow these steps:\n",
    "    # 1. Set model to training mode: model.train()\n",
    "    # 2. Loop over epochs\n",
    "    # 3. For each epoch, loop over batches from train_loader\n",
    "    # 4. For each batch:\n",
    "    #    a. Apply data augmentation: X_batch_aug = augment_batch(X_batch)\n",
    "    #    b. Zero gradients: optimizer.zero_grad()\n",
    "    #    c. Forward pass: outputs = model(X_batch_aug)\n",
    "    #    d. Compute loss: loss = criterion(outputs, y_batch)\n",
    "    #    e. Backward pass: loss.backward()\n",
    "    #    f. Clip gradients: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    #    g. Update weights: optimizer.step()\n",
    "    # 5. After each epoch, update learning rate: scheduler.step()\n",
    "    # 6. (Optional) Print training progress every 10 epochs\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement the training loop\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # TODO: EVALUATION (YOUR TASK)\n",
    "    # ============================================================================\n",
    "    # After training, evaluate the model on the test set:\n",
    "    # 1. Set model to evaluation mode: model.eval()\n",
    "    # 2. Use torch.no_grad() context\n",
    "    # 3. Create a DataLoader for test data (no shuffling needed)\n",
    "    # 4. Loop over test batches and collect predictions\n",
    "    # 5. Compute accuracy using the accuracy() function from utils\n",
    "    # 6. Return the test accuracy\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement the evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the deeper CNN implementation\n",
    "res = test_exercise_7_proper_cnn(student_proper_cnn)\n",
    "show_result(\"Exercise 3 \u2013 Proper CNN\", res)\n",
    "\n",
    "# Optional: test on the dataset generated above\n",
    "try:\n",
    "    acc = student_proper_cnn(X_train, y_train, X_test, y_test)\n",
    "    print(f\"Proper CNN accuracy: {acc:.3f}\")\n",
    "except NotImplementedError:\n",
    "    print(\"Implement student_proper_cnn above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion\n",
    "\n",
    "Briefly reflect on your results:\n",
    "\n",
    "- Did the deeper CNN outperform the baseline models? By how much?\n",
    "- What were the key techniques that helped achieve >50% accuracy?\n",
    "- Why is it important to compare against simple baselines?\n",
    "- How does PyTorch compare to implementing CNNs from scratch with NumPy?\n",
    "\n",
    "_Provide your answers here._\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}