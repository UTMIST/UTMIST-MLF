{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02389d47",
   "metadata": {},
   "source": [
    "\n",
    "# MLF Week 4: Neural Networks Part 2 - Training (Solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86be548",
   "metadata": {},
   "source": [
    "\n",
    "Reference version with filled code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf80340",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b094fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "import math, random, os, sys, time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2261fb4",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Backpropagation (Intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f599483",
   "metadata": {},
   "source": [
    "\n",
    "Autograd records the computation graph during the forward pass. Calling `loss.backward()` applies the chain rule through that graph and populates gradients for leaf tensors.\n",
    "\n",
    "You have seen this in week one, when we use to compute the gradient manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d25f4",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Autograd mini demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e609c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = (3*x + 2)**2 / 2\n",
    "y.backward()\n",
    "print(\"dy/dx at x=2:\", x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c4f3c",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data (two moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7419e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_toy_data(n_samples=1200, noise=0.2, test_size=0.2, seed=0):\n",
    "    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed)\n",
    "    return (torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.long),\n",
    "            torch.tensor(X_val,   dtype=torch.float32),\n",
    "            torch.tensor(y_val,   dtype=torch.long))\n",
    "\n",
    "X_train, y_train, X_val, y_val = make_toy_data()\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds   = TensorDataset(X_val, y_val)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad5c80",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model (MLP)\n",
    "Let's build a sample 3 layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_sizes=(32, 32), out_dim=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(last, h), nn.ReLU()]\n",
    "            last = h\n",
    "        layers += [nn.Linear(last, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP().to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268ada9",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ebbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader, device=device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=50, lr=0.05, optimizer_name='sgd', device=device):\n",
    "    if optimizer_name.lower() == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, val_losses, val_accs = [], [], []\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        train_losses.append(running / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vloss = 0.0\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                vloss += criterion(model(xb), yb).item()\n",
    "            vloss /= len(val_loader)\n",
    "        val_losses.append(vloss)\n",
    "        val_accs.append(accuracy(model, val_loader, device=device))\n",
    "\n",
    "        print(f\"Epoch {ep}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accs[-1]:.4f}\")\n",
    "\n",
    "    return {\"train_losses\": train_losses, \"val_losses\": val_losses, \"val_accs\": val_accs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e922fea",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Baseline run (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LR = 0.05\n",
    "OPT = 'sgd'  # or 'adam'\n",
    "\n",
    "model = MLP().to(device)\n",
    "out = train_model(model, train_loader, val_loader, epochs=EPOCHS, lr=LR, optimizer_name=OPT, device=device)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(out[\"train_losses\"], label=\"train\")\n",
    "plt.plot(out[\"val_losses\"], label=\"val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Loss ({OPT}, lr={LR})\")\n",
    "plt.legend(); plt.show()\n",
    "print(\"Validation accuracy:\", round(out[\"val_accs\"][-1], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000cdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    with torch.no_grad():\n",
    "        # Fix: Convert numpy arrays to tensors properly\n",
    "        Xgrid = torch.stack([torch.from_numpy(xx.ravel()), torch.from_numpy(yy.ravel())], dim=1).float()\n",
    "        Xgrid = Xgrid.to(device)  # Move to device\n",
    "        logits = model(Xgrid)\n",
    "        # Fix: Use softmax for multi-class, then take probability of class 1\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]  # Probability of class 1\n",
    "        Z = probs.cpu().view(xx.shape)  # Move back to CPU for plotting\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    plt.colorbar(label='Probability of Class 1')\n",
    "\n",
    "plot_decision_boundary(model, X_train, y_train)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis')\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2'); plt.title('Decision Boundary'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43475b",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Optimizers (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b13308",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\n",
    "    ('sgd', 0.01, 30),\n",
    "    ('sgd', 0.05, 30),\n",
    "    ('adam', 0.001, 30),\n",
    "    ('adam', 0.01, 30),\n",
    "]\n",
    "\n",
    "plt.figure()\n",
    "for opt, lr, epochs in settings:\n",
    "    model = MLP().to(device)\n",
    "    out = train_model(model, train_loader, val_loader, epochs=epochs, lr=lr, optimizer_name=opt, device=device)\n",
    "    plt.plot(out[\"val_losses\"], label=f\"{opt}, lr={lr}\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.title(\"Val Loss across settings\"); plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
