{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02389d47",
   "metadata": {},
   "source": [
    "\n",
    "# MLF Week 4: Neural Networks Part 2 - Training (Solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86be548",
   "metadata": {},
   "source": [
    "\n",
    "Reference version with filled code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf80340",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b094fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "import math, random, os, sys, time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2261fb4",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Backpropagation (Intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f599483",
   "metadata": {},
   "source": [
    "\n",
    "Autograd records the computation graph during the forward pass. Calling `loss.backward()` applies the chain rule through that graph and populates gradients for leaf tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d25f4",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Autograd mini demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e609c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = (3*x + 2)**2 / 2\n",
    "y.backward()\n",
    "print(\"dy/dx at x=2:\", x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c4f3c",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data (two moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7419e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_toy_data(n_samples=1200, noise=0.2, test_size=0.2, seed=0):\n",
    "    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed)\n",
    "    return (torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.long),\n",
    "            torch.tensor(X_val,   dtype=torch.float32),\n",
    "            torch.tensor(y_val,   dtype=torch.long))\n",
    "\n",
    "X_train, y_train, X_val, y_val = make_toy_data()\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds   = TensorDataset(X_val, y_val)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad5c80",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_sizes=(32, 32), out_dim=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(last, h), nn.ReLU()]\n",
    "            last = h\n",
    "        layers += [nn.Linear(last, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP().to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268ada9",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ebbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader, device=device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=50, lr=0.05, optimizer_name='sgd', device=device):\n",
    "    if optimizer_name.lower() == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, val_losses, val_accs = [], [], []\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "\n",
    "        train_losses.append(running / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vloss = 0.0\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                vloss += criterion(model(xb), yb).item()\n",
    "            vloss /= len(val_loader)\n",
    "        val_losses.append(vloss)\n",
    "        val_accs.append(accuracy(model, val_loader, device=device))\n",
    "\n",
    "    return {\"train_losses\": train_losses, \"val_losses\": val_losses, \"val_accs\": val_accs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e922fea",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Baseline run (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LR = 0.05\n",
    "OPT = 'sgd'  # or 'adam'\n",
    "\n",
    "model = MLP().to(device)\n",
    "out = train_model(model, train_loader, val_loader, epochs=EPOCHS, lr=LR, optimizer_name=OPT, device=device)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(out[\"train_losses\"], label=\"train\")\n",
    "plt.plot(out[\"val_losses\"], label=\"val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Loss ({OPT}, lr={LR})\")\n",
    "plt.legend(); plt.show()\n",
    "print(\"Validation accuracy:\", round(out[\"val_accs\"][-1], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43475b",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Optimizers (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b13308",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\n",
    "    ('sgd', 0.01, 30),\n",
    "    ('sgd', 0.05, 30),\n",
    "    ('adam', 0.001, 30),\n",
    "    ('adam', 0.01, 30),\n",
    "]\n",
    "\n",
    "plt.figure()\n",
    "for opt, lr, epochs in settings:\n",
    "    model = MLP().to(device)\n",
    "    out = train_model(model, train_loader, val_loader, epochs=epochs, lr=lr, optimizer_name=opt, device=device)\n",
    "    plt.plot(out[\"val_losses\"], label=f\"{opt}, lr={lr}\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.title(\"Val Loss across settings\"); plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
