{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fb5fb0",
   "metadata": {},
   "source": [
    "\n",
    "# MLF Week 4: Neural Networks Part 2 - Training\n",
    "\n",
    "The notebook accompanies the **MLF Week 4 Slides** and extends Week 3 by focusing on **how neural networks learn**. We connect the ideas of **backpropagation**, **autograd**, and a clean **training loop**. We also compare **SGD** and **Adam** and run small **hyperparameter** and **architecture** experiments.\n",
    "\n",
    "**You will:**\n",
    "- Describe backpropagation at a high level (no derivations).\n",
    "- Use **PyTorch autograd** to obtain gradients automatically.\n",
    "- Implement the standard **training loop** (forward → loss → backward → step).\n",
    "- Compare **SGD vs Adam**, tune **learning rate**, and read **loss curves**.\n",
    "- Try different **depth/width** settings on the same 2D dataset from Week 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd2bd7",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup\n",
    "\n",
    "We’ll use the same helper utilities and data setup as in previous weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "import math, random, os, sys, time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475c63a",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Backpropagation (Intuition)\n",
    "\n",
    "**Big picture.** A neural network is a chain of simple computations. The **forward pass** combines inputs with weights to make a prediction and compute a single **loss** value. To learn, we need to know **how changing each weight changes the loss**.\n",
    "\n",
    "**Backpropagation** applies the **chain rule** in reverse through this chain. Layers close to the output get a clear signal first; earlier layers receive a signal that reflects how much they contributed to the error. The result is one gradient per parameter.\n",
    "\n",
    "We rarely compute these derivatives by hand. Instead, we rely on **automatic differentiation** (autograd). PyTorch records operations during the forward pass and, when we call `loss.backward()`, it traverses the graph and fills in the gradients for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc16bd",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Autograd mini demo\n",
    "\n",
    "This tiny example shows the mechanism on a simple scalar function.\n",
    "\n",
    "**Task**\n",
    "1. Create a tensor with `requires_grad=True`.\n",
    "2. Build a simple expression from it.\n",
    "3. Call `.backward()` on the result.\n",
    "4. Inspect `.grad` on the input tensor.\n",
    "\n",
    "**Why this matters.** If this works for a scalar function, the same idea scales to millions of parameters in a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf51cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a small scalar example that uses autograd.\n",
    "# Example steps:\n",
    "# x = torch.tensor([2.0], requires_grad=True)\n",
    "# y = (3*x + 2)**2 / 2\n",
    "# y.backward()\n",
    "# print(\"dy/dx:\", x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3693f",
   "metadata": {},
   "source": [
    "\n",
    "**Common Mistakes**\n",
    "- Forgetting `requires_grad=True` means gradients stay `None`.\n",
    "- Calling `.backward()` more than once on the same graph without `retain_graph=True`.\n",
    "- Using `.item()` on a tensor and expecting backprop through the Python float (breaks the graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f6cd0",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "\n",
    "We’ll use the **two-moons** dataset for binary classification.  \n",
    "It’s a simple 2D dataset with non-linear boundaries, making it ideal for visualizing how neural networks learn complex patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_toy_data(n_samples=1200, noise=0.2, test_size=0.2, seed=0):\n",
    "    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed)\n",
    "    return (torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.long),\n",
    "            torch.tensor(X_val,   dtype=torch.float32),\n",
    "            torch.tensor(y_val,   dtype=torch.long))\n",
    "\n",
    "X_train, y_train, X_val, y_val = make_toy_data()\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds   = TensorDataset(X_val, y_val)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c131243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccf11e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model (MLP)\n",
    "\n",
    "We reuse the Week 3 **MLP**: a stack of `Linear` layers with **ReLU**. This model is expressive enough to learn a curved decision boundary but still simple to read.\n",
    "\n",
    "> Keeping the model familiar lets us focus on training details and optimizer behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f82c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_sizes=(32, 32), out_dim=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(last, h), nn.ReLU()]\n",
    "            last = h\n",
    "        layers += [nn.Linear(last, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP().to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc1432",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Training loop\n",
    "\n",
    "We will use **`nn.CrossEntropyLoss`** for classification. For each mini-batch:\n",
    "\n",
    "1. **Zero** old gradients (`optimizer.zero_grad()`).\n",
    "2. **Forward**: pass inputs through the model to get `logits`.\n",
    "3. **Loss**: compare logits against labels.\n",
    "4. **Backward**: `loss.backward()` computes all gradients.\n",
    "5. **Step**: `optimizer.step()` updates parameters.\n",
    "\n",
    "We also compute **validation loss** and **accuracy** per epoch to monitor learning and detect over/underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a857cd6",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Implement helpers\n",
    "\n",
    "Fill in `accuracy()` and `train_model()`. Keep the code clear and minimal. Return curves so you can plot them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd135086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement accuracy() and train_model().\n",
    "\n",
    "def accuracy(model, loader, device=device):\n",
    "    # Compute classification accuracy over a DataLoader.\n",
    "    # Hint: pred = logits.argmax(dim=1); compare to yb; average across dataset.\n",
    "    pass\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=50, lr=0.05, optimizer_name='sgd', device=device):\n",
    "    # Create optimizer (SGD or Adam). For each epoch, loop over batches:\n",
    "    # zero_grad → forward → loss → backward → step. Track train/val loss and val acc.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd641129",
   "metadata": {},
   "source": [
    "\n",
    "**Debug checklist**\n",
    "- If loss is not decreasing at all, then lower or raise LR slightly (e.g., 0.1 → 0.05 or 0.01).\n",
    "- If `nan` loss, then reduce LR; print a single batch to check labels and shapes.\n",
    "- If Val accuracy stuck at ~50%, increase capacity a bit or try Adam with smaller LR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a102f59",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Baseline run (SGD)\n",
    "\n",
    "Start with **SGD** and a reasonable learning rate. Plot **train vs val loss** to check that the model is learning and not overfitting immediately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train once with SGD and plot loss curves.\n",
    "EPOCHS = 50\n",
    "LR = 0.05\n",
    "OPT = 'sgd'  # 'sgd' or 'adam'\n",
    "\n",
    "model = MLP().to(device)\n",
    "# out = train_model(model, train_loader, val_loader, epochs=EPOCHS, lr=LR, optimizer_name=OPT, device=device)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(out[\"train_losses\"], label=\"train\")\n",
    "# plt.plot(out[\"val_losses\"], label=\"val\")\n",
    "# plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Loss ({OPT}, lr={LR})\")\n",
    "# plt.legend(); plt.show()\n",
    "# print(\"Validation accuracy:\", round(out[\"val_accs\"][-1], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e5293",
   "metadata": {},
   "source": [
    "\n",
    "**Reflect**\n",
    "- Does the **validation loss** follow the training loss downwards?\n",
    "- If training loss falls but validation loss climbs, you may be **overfitting** (reduce epochs or capacity).\n",
    "- If both losses are flat, try a different LR or optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    with torch.no_grad():\n",
    "        # Fix: Convert numpy arrays to tensors properly\n",
    "        Xgrid = torch.stack([torch.from_numpy(xx.ravel()), torch.from_numpy(yy.ravel())], dim=1).float()\n",
    "        Xgrid = Xgrid.to(device)  # Move to device\n",
    "        logits = model(Xgrid)\n",
    "        # Fix: Use softmax for multi-class, then take probability of class 1\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]  # Probability of class 1\n",
    "        Z = probs.cpu().view(xx.shape)  # Move back to CPU for plotting\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    plt.colorbar(label='Probability of Class 1')\n",
    "\n",
    "plot_decision_boundary(model, X_train, y_train)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis')\n",
    "plt.xlabel('Feature 1'); plt.ylabel('Feature 2'); plt.title('Decision Boundary'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5405f",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Optimizers (SGD vs Adam)\n",
    "\n",
    "- **SGD** uses one global learning rate for all parameters. It often needs careful LR tuning but can generalize well.\n",
    "- **Adam** adapts the step size per-parameter using moving averages of gradients; it often **converges faster** with a smaller LR (e.g., `1e-3`).\n",
    "\n",
    "We will compare a few settings and plot the curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare a few settings and plot validation loss for each.\n",
    "# Example:\n",
    "# settings = [('sgd', 0.01, 30), ('sgd', 0.05, 30), ('adam', 0.001, 30), ('adam', 0.01, 30)]\n",
    "# plt.figure()\n",
    "# for opt, lr, epochs in settings:\n",
    "#     model = MLP().to(device)\n",
    "#     out = train_model(model, train_loader, val_loader, epochs=epochs, lr=lr, optimizer_name=opt, device=device)\n",
    "#     plt.plot(out[\"val_losses\"], label=f\"{opt}, lr={lr}\")\n",
    "# plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.title(\"Val Loss across settings\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d095e",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation tips**\n",
    "- The **lower** the validation curve and the **faster** it drops, the better the setting.\n",
    "- Very noisy or exploding curves often indicate an LR that is **too high**.\n",
    "- If the curve descends slowly, increase epochs or try a slightly **larger** LR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040cf9d",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Practical tips\n",
    "\n",
    "- **Initialization.** PyTorch defaults are good for ReLU MLPs.\n",
    "- **Batch size.** 32–128 are reasonable; smaller batches add gradient noise that can help generalization.\n",
    "- **Learning rate.** The most sensitive hyperparameter. Start around `1e-3` for Adam, `1e-2`–`1e-1` for SGD.\n",
    "- **Overfitting.** Compare **train vs val** loss across epochs; consider early stopping when val loss rises.\n",
    "- **Sanity checks.** Overfit a tiny subset (e.g., 100 samples) to verify your loop works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663cc71",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, you trained a simple neural network end-to-end using **PyTorch’s autograd** and **optimizers**.  \n",
    "You saw how a forward pass, loss calculation, backward pass, and optimizer step work together to make the model learn."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
